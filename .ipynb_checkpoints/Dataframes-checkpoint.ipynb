{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/Users/resitkadir/spark/spark-2.4.6/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark session olusturuyoruz\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aşağıdaki ayarları bilgisayarınızın belleğine göre değiştirebilirsiniz\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName(\"Create-Dataframe\") \\\n",
    "        .config(\"spark.executor.memory\",\"6g\") \\\n",
    "        .config(\"spark.driver.memory\",\"2g\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# sparkContext'i kısaltmada tut\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# listeden dataframe oluşturmak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Row rdd yapiyoruz \n",
    "#x i al row yap\n",
    "from pyspark.sql import Row\n",
    "list_rdd = sc.parallelize([1,2,3,4,5,6,4,5]).map(lambda x: Row(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sütun ismi bir tane bile olsa Python listesi olarak parametre verilir\n",
    "#dataframe ceviriyoruz burada\n",
    "df_from_list = list_rdd.toDF(['rakamlar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|rakamlar|\n",
      "+--------+\n",
      "|       1|\n",
      "|       2|\n",
      "|       3|\n",
      "|       4|\n",
      "|       5|\n",
      "|       6|\n",
      "|       4|\n",
      "|       5|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_list.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# range ile dataframe yaratmak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yöntem-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_range = sc.parallelize(range(10,100,5)). \\\n",
    "                map(lambda x: (x,)). \\\n",
    "                toDF([\"range\"])\n",
    "#lambda x sonucu range basligi altina at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|range|\n",
      "+-----+\n",
      "|   10|\n",
      "|   15|\n",
      "|   20|\n",
      "|   25|\n",
      "+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_range.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yöntem-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "#spark session ile yapalim ()\n",
    "df_from_range2 =spark.createDataFrame(range(10,100,5), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|   10|\n",
      "|   15|\n",
      "|   20|\n",
      "+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_range2.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dosyadan veri okuyarak Dataframe oluşturmak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_file = spark.read.csv(\"data/OnlineRetail.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 _c0|\n",
      "+--------------------+\n",
      "|InvoiceNo;StockCo...|\n",
      "|536365;85123A;WHI...|\n",
      "|536365;71053;WHIT...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_file.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_file =  spark.read. \\\n",
    "                option(\"sep\",\";\"). \\\n",
    "                csv(\"/Users/resitkadir/ApacheSpark/data/OnlineRetail.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|      _c0|      _c1|                 _c2|     _c3|            _c4|      _c5|       _c6|           _c7|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|    InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|1.12.2010 08:26|     2,55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|1.12.2010 08:26|     3,39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_file.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_file.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_file =  spark.read. \\\n",
    "                option(\"sep\",\";\"). \\\n",
    "                option(\"header\",\"True\"). \\\n",
    "                option(\"inferSchema\",\"True\"). \\\n",
    "                csv(\"/Users/resitkadir/ApacheSpark/data/OnlineRetail.csv\")\n",
    "                #interScheman veri turleri duzelltti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|    InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|1.12.2010 08:26|     2,55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|1.12.2010 08:26|     3,39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|1.12.2010 08:26|     2,75|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_file.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: string (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print schema ,dataframe rdd farkli oldugu icin, en onemli farki bu\n",
    "df_from_file.printSchema()\n",
    "#veri turleri hepsi string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Action yapalim\n",
    "df_from_file.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|InvoiceNo|StockCode|\n",
      "+---------+---------+\n",
      "|   536365|   85123A|\n",
      "|   536365|    71053|\n",
      "|   536365|   84406B|\n",
      "|   536365|   84029G|\n",
      "|   536365|   84029E|\n",
      "|   536365|    22752|\n",
      "|   536365|    21730|\n",
      "|   536366|    22633|\n",
      "|   536366|    22632|\n",
      "|   536367|    84879|\n",
      "+---------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select ile islem yapalim\n",
    "\n",
    "df_from_file.select(\"InvoiceNo\",\"StockCode\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|    InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|1.12.2010 08:26|     3,39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|1.12.2010 08:26|     3,39|     17850|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|1.12.2010 08:26|     4,25|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|1.12.2010 08:26|     3,39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|1.12.2010 08:26|     2,75|     17850|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|1.12.2010 08:26|     7,65|     17850|United Kingdom|\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|1.12.2010 08:26|     2,55|     17850|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|1.12.2010 08:28|     1,85|     17850|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|1.12.2010 08:28|     1,85|     17850|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|1.12.2010 08:34|     1,69|     13047|United Kingdom|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sort\n",
    "df_from_file.sort(\"InvoiceNo\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [Quantity#153 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(Quantity#153 ASC NULLS FIRST, 200)\n",
      "   +- *(1) FileScan csv [InvoiceNo#150,StockCode#151,Description#152,Quantity#153,InvoiceDate#154,UnitPrice#155,CustomerID#156,Country#157] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/resitkadir/ApacheSpark/data/OnlineRetail.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<InvoiceNo:string,StockCode:string,Description:string,Quantity:int,InvoiceDate:string,UnitP...\n"
     ]
    }
   ],
   "source": [
    "df_from_file.sort(\"Quantity\").explain()\n",
    "#spark in neler yapdigihi explain ediyor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dinamik conf ayarı ve shuffle partition sayısını değiştirme\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\",\"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [Quantity#153 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(Quantity#153 ASC NULLS FIRST, 5)\n",
      "   +- *(1) FileScan csv [Description#152,Quantity#153] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/resitkadir/ApacheSpark/data/OnlineRetail.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Description:string,Quantity:int>\n"
     ]
    }
   ],
   "source": [
    "# Yeni conf ile sort\n",
    "df_from_file.select(\"Description\",\"Quantity\").sort(\"Quantity\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [Quantity#153 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(Quantity#153 ASC NULLS FIRST, 5)\n",
      "   +- *(1) FileScan csv [Description#152,Quantity#153] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/resitkadir/ApacheSpark/data/OnlineRetail.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Description:string,Quantity:int>\n"
     ]
    }
   ],
   "source": [
    "# Dinamik conf ayarı ve shuffle partition sayısını değiştirme\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\",\"5\")\n",
    "# Yeni conf ile sort\n",
    "df_from_file.select(\"Description\",\"Quantity\").sort(\"Quantity\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "# Create SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "# Aşağıdaki ayarları bilgisayarınızın belleğine göre değiştirebilirsiniz\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName(\"Dataframe-WordCount\") \\\n",
    "        .config(\"spark.executor.memory\",\"6g\") \\\n",
    "        .config(\"spark.driver.memory\",\"2g\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# sparkContext'i kısaltmada tut\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "veri_dosyasi = \"data/omer_seyfettin_forsa_hikaye.txt\"\n",
    "\n",
    "\n",
    "hikaye_df = spark.read.text(veri_dosyasi)\n",
    "#text , textfile degil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------+\n",
      "|value                                                                                 |\n",
      "+--------------------------------------------------------------------------------------+\n",
      "|Ömer Seyfettin -         Forsa                                                        |\n",
      "|                                                                                      |\n",
      "|Akdeniz’in, kahramanlık yuvası sonsuz ufuklarına bakan küçük tepe, minimini bir çiçek |\n",
      "+--------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hikaye_df.show(3,truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD'de flatMap ile boşluklardan kelimeleri ayırıp aşağı atıyorduk. \n",
    "### Yapısal API'de benzer işi sql explode fonksiyonu ile yapıyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|    value|\n",
      "+---------+\n",
      "|     Ömer|\n",
      "|Seyfettin|\n",
      "|        -|\n",
      "|         |\n",
      "|         |\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, split, col\n",
    "# Her bir kelimeyi boşluklarla ayıralım ve başka bir rdd'de tutalım\n",
    "kelimeler = hikaye_df.select(explode(split(col(\"value\"), \" \")).alias(\"value\"))\n",
    "#explode ile secim yapdik,icine col ile value yazdik,split ilede split fonk ile yazdik,boslukla ayir\n",
    "#alias(as demek numpy as gibi) select in icinde olmasi lazim, ve isim ver\n",
    "kelimeler.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|value|count|\n",
      "+-----+-----+\n",
      "|     |   85|\n",
      "|  bir|   32|\n",
      "|    –|   31|\n",
      "|  yıl|    8|\n",
      "| diye|    6|\n",
      "| onun|    5|\n",
      "| Kırk|    5|\n",
      "|dedi.|    5|\n",
      "| Türk|    5|\n",
      "|doğru|    5|\n",
      "+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#grouoby ile sayalim\n",
    "from pyspark.sql.functions import desc\n",
    "kelimeler.groupBy(\"value\").count().orderBy(desc(\"count\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|value|count(value)|\n",
      "+-----+------------+\n",
      "|     |          85|\n",
      "|  bir|          32|\n",
      "|    –|          31|\n",
      "|  yıl|           8|\n",
      "| diye|           6|\n",
      "| onun|           5|\n",
      "| Kırk|           5|\n",
      "|dedi.|           5|\n",
      "|doğru|           5|\n",
      "| Türk|           5|\n",
      "+-----+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ikinci yontem\n",
    "kelimeler.groupBy(\"value\").agg({\"value\": \"count\"}).orderBy(desc(\"count(value)\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-CsvDosyasinaSQLAtmak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#session olustur\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[4]\") \\\n",
    ".appName(\"Sql-Olusturmak\") \\\n",
    ".config(\"spark.executor.memory\",\"4g\") \\\n",
    ".config(\"spark.driver.memory\",\"2g\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datayi oku,dataframe olusturduk\n",
    "retailDF = spark.read \\\n",
    ".option(\"header\",\"True\") \\\n",
    ".option(\"inferSchema\",\"True\") \\\n",
    ".option(\"sep\",\";\") \\\n",
    ".csv(\"/Users/resitkadir/ApacheSpark/data/OnlineRetail.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanal bir tablo olusturduk\n",
    "retailDF.createOrReplaceTempView(\"tablo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|    InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|1.12.2010 08:26|     2,55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|1.12.2010 08:26|     3,39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|1.12.2010 08:26|     2,75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|1.12.2010 08:26|     3,39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|1.12.2010 08:26|     3,39|     17850|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|1.12.2010 08:26|     7,65|     17850|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|1.12.2010 08:26|     4,25|     17850|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|1.12.2010 08:28|     1,85|     17850|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|1.12.2010 08:28|     1,85|     17850|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|1.12.2010 08:34|     1,69|     13047|United Kingdom|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|1.12.2010 08:34|      2,1|     13047|United Kingdom|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6|1.12.2010 08:34|      2,1|     13047|United Kingdom|\n",
      "|   536367|    22749|FELTCRAFT PRINCES...|       8|1.12.2010 08:34|     3,75|     13047|United Kingdom|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6|1.12.2010 08:34|     1,65|     13047|United Kingdom|\n",
      "|   536367|    84969|BOX OF 6 ASSORTED...|       6|1.12.2010 08:34|     4,25|     13047|United Kingdom|\n",
      "|   536367|    22623|BOX OF VINTAGE JI...|       3|1.12.2010 08:34|     4,95|     13047|United Kingdom|\n",
      "|   536367|    22622|BOX OF VINTAGE AL...|       2|1.12.2010 08:34|     9,95|     13047|United Kingdom|\n",
      "|   536367|    21754|HOME BUILDING BLO...|       3|1.12.2010 08:34|     5,95|     13047|United Kingdom|\n",
      "|   536367|    21755|LOVE BUILDING BLO...|       3|1.12.2010 08:34|     5,95|     13047|United Kingdom|\n",
      "|   536367|    21777|RECIPE BOX WITH M...|       4|1.12.2010 08:34|     7,95|     13047|United Kingdom|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sql yeri actik\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "SELECT*FROM tablo\n",
    "\n",
    "\"\"\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|       Country|UnitPrice|\n",
      "+--------------+---------+\n",
      "|United Kingdom|  94911.0|\n",
      "|          EIRE|   9423.0|\n",
      "|       Germany|   7930.0|\n",
      "|        France|   6288.0|\n",
      "|         Spain|   2927.0|\n",
      "|       Finland|   1578.0|\n",
      "|       Belgium|   1503.0|\n",
      "|        Norway|   1451.0|\n",
      "|   Switzerland|   1267.0|\n",
      "|        Sweden|    921.0|\n",
      "+--------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#groupby yapalim\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "SELECT Country ,SUM(UnitPrice) UnitPrice\n",
    "FROM tablo\n",
    "GROUP BY Country\n",
    "ORDER BY UnitPrice DESC\n",
    "\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eger bir datasetini sik kullaniyorsak cacheleyerek daha hizli islem yapabiliriz, yontemi ise\n",
    "retailDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-String Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/Users/resitkadir/spark/spark-2.4.6/\")\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    ".master(\"local[2]\") \\\n",
    ".appName(\"StringOps\") \\\n",
    ".config(\"spark.executor.memory\",\"4g\") \\\n",
    ".config(\"spark.driver.memory\",\"2g\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_df = spark.read \\\n",
    "            .option(\"header\",\"True\") \\\n",
    "            .option(\"inferSchema\",\"True\") \\\n",
    "            .option(\"sep\",\",\") \\\n",
    "            .csv(\"/Users/resitkadir/ApacheSpark/data/simple_dirty_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---+--------+-----------+-----------+-----------+--------------------+\n",
      "|sirano|     isim|yas|cinsiyet|     meslek|      sehir|aylik_gelir|            mal_mulk|\n",
      "+------+---------+---+--------+-----------+-----------+-----------+--------------------+\n",
      "|     1|    Cemal| 35|       E|       Isci|     Ankara|     3500.0|               araba|\n",
      "|     2|   ceyda | 42|       K|      Memur|    Kayseri|     4200.0|            araba|ev|\n",
      "|     3|    Timur| 30|    null|   Müzüsyen|Istanbul   |     9000.0|     araba|ev|yazlık|\n",
      "|     4|   Burcu | 29|       K|Pazarlamacı|     Ankara|     4200.0|               araba|\n",
      "|     5|  Yasemin| 23|       K|Pazarlamaci|      Bursa|     4800.0|               araba|\n",
      "|     6|      Ali| 33|       E|      Memur|     Ankara|     4250.0|                  ev|\n",
      "|     7|    Dilek| 29|       K|Pazarlamaci|   Istanbul|     7300.0|        araba|yazlık|\n",
      "|     8|    Murat| 31|       E|   Müzüsyen|   Istanbul|    12000.0|araba|ev|dükkan|y...|\n",
      "|     9|    Ahmet| 33|       E|     Doktor|     Ankara|   180000.0|     araba|ev|yazlık|\n",
      "|    10| Muhittin| 46|       E|     Berber|   Istanbul|    12000.0|     araba|ev|dükkan|\n",
      "|    11| Hicaziye| 47|       K| Tuhafiyeci|       null|        4.8|              dükkan|\n",
      "|    12|    Harun| 43|       E|    Tornacı|    Ankara |     4200.0|               araba|\n",
      "|    13|    hakkı| 33|       E|      memur|     Çorum |     3750.0|                  ev|\n",
      "|    14|  Gülizar| 37|       K|     Doktor|      İzmir|    14250.0|               araba|\n",
      "|    15|   Şehmuz| 41|       E|   Müzisyen|     Ankara|     8700.0|               araba|\n",
      "+------+---------+---+--------+-----------+-----------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Datafram'e göz atalım\n",
    "simple_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bu ders tamamen sql string ops ile ilgili o yüzden hepsini indirelim\n",
    "#from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import explode, split, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---+--------+-----------+-----------+-----------+---------------+------------------------+\n",
      "|sirano|isim   |yas|cinsiyet|meslek     |sehir      |aylik_gelir|mal_mulk       |aylik_gelir_format      |\n",
      "+------+-------+---+--------+-----------+-----------+-----------+---------------+------------------------+\n",
      "|1     |Cemal  |35 |E       |Isci       |Ankara     |3500.0     |araba          |Isci - Ankara           |\n",
      "|2     |ceyda  |42 |K       |Memur      |Kayseri    |4200.0     |araba|ev       |Memur - Kayseri         |\n",
      "|3     |Timur  |30 |null    |Müzüsyen   |Istanbul   |9000.0     |araba|ev|yazlık|Müzüsyen - Istanbul     |\n",
      "|4     |Burcu  |29 |K       |Pazarlamacı|    Ankara |4200.0     |araba          |Pazarlamacı -     Ankara|\n",
      "|5     |Yasemin|23 |K       |Pazarlamaci|Bursa      |4800.0     |araba          |Pazarlamaci - Bursa     |\n",
      "+------+-------+---+--------+-----------+-----------+-----------+---------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Concat iki stringi birlestiren fonksiyondur\n",
    "\n",
    "# Meslek ve şehir birleştirme. Araya birşey eklemek için lit() fonksiyonu kullanırız.\n",
    "#yeni dataframe olustircaz\n",
    "#meslek ile sehri concat edelim ve yeni sutun yapalaim \n",
    "df_concat = simple_df.withColumn(\"aylik_gelir_format\", F.concat(col(\"meslek\"), F.lit(\" - \"), col(\"sehir\")))\n",
    "df_concat.show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-  number formatt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---+--------+-----------+-----------+-----------+---------------+------------------+\n",
      "|sirano|isim   |yas|cinsiyet|meslek     |sehir      |aylik_gelir|mal_mulk       |aylik_gelir_format|\n",
      "+------+-------+---+--------+-----------+-----------+-----------+---------------+------------------+\n",
      "|1     |Cemal  |35 |E       |Isci       |Ankara     |3500.0     |araba          |3,500.00          |\n",
      "|2     |ceyda  |42 |K       |Memur      |Kayseri    |4200.0     |araba|ev       |4,200.00          |\n",
      "|3     |Timur  |30 |null    |Müzüsyen   |Istanbul   |9000.0     |araba|ev|yazlık|9,000.00          |\n",
      "|4     |Burcu  |29 |K       |Pazarlamacı|    Ankara |4200.0     |araba          |4,200.00          |\n",
      "|5     |Yasemin|23 |K       |Pazarlamaci|Bursa      |4800.0     |araba          |4,800.00          |\n",
      "+------+-------+---+--------+-----------+-----------+-----------+---------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# numara formatlama,virgulden sonra iki basamak yapalim\n",
    "#yeni sutun ismi aylik gelir format olsun\n",
    "df_number_format = simple_df.withColumn(\"aylik_gelir_format\", F.format_number(col(\"aylik_gelir\"),2))\n",
    "df_number_format.show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. lower, initcap, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---+--------+-----------+-----------+-----------+---------------+------------+------------+------------+\n",
      "|sirano|isim   |yas|cinsiyet|meslek     |sehir      |aylik_gelir|mal_mulk       |meslek_lower|isim_initcap|sehir_length|\n",
      "+------+-------+---+--------+-----------+-----------+-----------+---------------+------------+------------+------------+\n",
      "|1     |Cemal  |35 |E       |Isci       |Ankara     |3500.0     |araba          |isci        |Cemal       |6           |\n",
      "|2     |ceyda  |42 |K       |Memur      |Kayseri    |4200.0     |araba|ev       |memur       |Ceyda       |7           |\n",
      "|3     |Timur  |30 |null    |Müzüsyen   |Istanbul   |9000.0     |araba|ev|yazlık|müzüsyen    |Timur       |11          |\n",
      "|4     |Burcu  |29 |K       |Pazarlamacı|    Ankara |4200.0     |araba          |pazarlamacı |Burcu       |10          |\n",
      "|5     |Yasemin|23 |K       |Pazarlamaci|Bursa      |4800.0     |araba          |pazarlamaci |Yasemin     |5           |\n",
      "+------+-------+---+--------+-----------+-----------+-----------+---------------+------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# meslek küçük harf, isimlerin ilk harfi büyük, sehirlerin kelime uzunlukları\n",
    "\n",
    "df_lower = simple_df \\\n",
    "        .withColumn(\"meslek_lower\", F.lower(col(\"meslek\"))) \\\n",
    "        .withColumn(\"isim_initcap\", F.initcap(col(\"isim\"))) \\\n",
    "        .withColumn(\"sehir_length\", F.length(col(\"sehir\")))\n",
    "\n",
    "# meslekler kucuk harfli olsun,isimlerinde(ilk harfi buyuk yaz) , ve sheir kelimesini uzunlugu nedir\n",
    "df_lower.show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-Trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---+--------+-----------+-----------+-----------+---------------+-----------+-----------+----------+\n",
      "|sirano|isim   |yas|cinsiyet|meslek     |sehir      |aylik_gelir|mal_mulk       |sehir_rtrim|sehir_ltrim|sehir_trim|\n",
      "+------+-------+---+--------+-----------+-----------+-----------+---------------+-----------+-----------+----------+\n",
      "|1     |Cemal  |35 |E       |Isci       |Ankara     |3500.0     |araba          |Ankara     |Ankara     |Ankara    |\n",
      "|2     |ceyda  |42 |K       |Memur      |Kayseri    |4200.0     |araba|ev       |Kayseri    |Kayseri    |Kayseri   |\n",
      "|3     |Timur  |30 |null    |Müzüsyen   |Istanbul   |9000.0     |araba|ev|yazlık|Istanbul   |Istanbul   |Istanbul  |\n",
      "|4     |Burcu  |29 |K       |Pazarlamacı|    Ankara |4200.0     |araba          |    Ankara |Ankara     |Ankara    |\n",
      "|5     |Yasemin|23 |K       |Pazarlamaci|Bursa      |4800.0     |araba          |Bursa      |Bursa      |Bursa     |\n",
      "+------+-------+---+--------+-----------+-----------+-----------+---------------+-----------+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Trim bosluk olan yerleri bak ankara ornegi ustteki gibi(10)\n",
    "\n",
    "#rt-rightrim\n",
    "df_trim = simple_df \\\n",
    "            .withColumn(\"sehir_rtrim\", F.rtrim(col(\"sehir\"))) \\\n",
    "            .withColumn(\"sehir_ltrim\", F.ltrim(col(\"sehir\"))) \\\n",
    "            .withColumn(\"sehir_trim\", F.trim(col(\"sehir\")))\n",
    "\n",
    "df_trim.show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-Replace and Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---+--------+-----------+-----------+-----------+---------------+-----------+-------------------+-------------------+\n",
      "|sirano|isim   |yas|cinsiyet|meslek     |sehir      |aylik_gelir|mal_mulk       |sehir_ist  |mal_mulk_split     |mal_mulk_ilk_eleman|\n",
      "+------+-------+---+--------+-----------+-----------+-----------+---------------+-----------+-------------------+-------------------+\n",
      "|1     |Cemal  |35 |E       |Isci       |Ankara     |3500.0     |araba          |Ankara     |[araba]            |araba              |\n",
      "|2     |ceyda  |42 |K       |Memur      |Kayseri    |4200.0     |araba|ev       |Kayseri    |[araba, ev]        |araba              |\n",
      "|3     |Timur  |30 |null    |Müzüsyen   |Istanbul   |9000.0     |araba|ev|yazlık|İSTanbul   |[araba, ev, yazlık]|araba              |\n",
      "|4     |Burcu  |29 |K       |Pazarlamacı|    Ankara |4200.0     |araba          |    Ankara |[araba]            |araba              |\n",
      "|5     |Yasemin|23 |K       |Pazarlamaci|Bursa      |4800.0     |araba          |Bursa      |[araba]            |araba              |\n",
      "+------+-------+---+--------+-----------+-----------+-----------+---------------+-----------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_replace = simple_df \\\n",
    "            .withColumn(\"sehir_ist\", F.regexp_replace(col(\"sehir\"), \"Ist\", \"İST\")) \\\n",
    "            .withColumn(\"mal_mulk_split\", F.split(col(\"mal_mulk\"), \"\\\\|\")) \\\n",
    "            .withColumn(\"mal_mulk_ilk_eleman\", col(\"mal_mulk_split\")[0])\n",
    "            \n",
    "    #sehir_ist diye bir column ac ve Ist yazilan yeri IST ile degistir\n",
    "    #F split ile split ettik\n",
    "    # ve ilk elemanlari al\n",
    "df_replace.show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sirano: integer (nullable = true)\n",
      " |-- isim: string (nullable = true)\n",
      " |-- yas: integer (nullable = true)\n",
      " |-- cinsiyet: string (nullable = true)\n",
      " |-- meslek: string (nullable = true)\n",
      " |-- sehir: string (nullable = true)\n",
      " |-- aylik_gelir: double (nullable = true)\n",
      " |-- mal_mulk: string (nullable = true)\n",
      " |-- sehir_ist: string (nullable = true)\n",
      " |-- mal_mulk_split: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- mal_mulk_ilk_eleman: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_replace.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-8d3513b7698b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 -  pyspark_df_to_disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***#ETL (Extract-Transform-Load) --> Cek ,donustur,yukle***\n",
    "\n",
    "***#kirli veri(Kaynak)--->ETL --->Temiz veri (Hedef)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    ".master(\"local[4]\") \\\n",
    ".appName(\"Csv-Üzeri-SQL\") \\\n",
    ".config(\"spark.executor.memory\",\"4g\") \\\n",
    ".config(\"spark.driver.memory\",\"2g\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    ".option(\"header\",\"True\") \\\n",
    ".option(\"inferSchema\",\"True\") \\\n",
    ".option(\"sep\",\",\") \\\n",
    ".csv(\"/Users/resitkadir/ApacheSpark/data/simple_dirty_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---+--------+-----------+-----------+-----------+---------------+\n",
      "|sirano|   isim|yas|cinsiyet|     meslek|      sehir|aylik_gelir|       mal_mulk|\n",
      "+------+-------+---+--------+-----------+-----------+-----------+---------------+\n",
      "|     1|  Cemal| 35|       E|       Isci|     Ankara|     3500.0|          araba|\n",
      "|     2| ceyda | 42|       K|      Memur|    Kayseri|     4200.0|       araba|ev|\n",
      "|     3|  Timur| 30|    null|   Müzüsyen|Istanbul   |     9000.0|araba|ev|yazlık|\n",
      "|     4| Burcu | 29|       K|Pazarlamacı|     Ankara|     4200.0|          araba|\n",
      "|     5|Yasemin| 23|       K|Pazarlamaci|      Bursa|     4800.0|          araba|\n",
      "+------+-------+---+--------+-----------+-----------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---+--------+-----------+--------+-----------+---------------+\n",
      "|sirano|   isim|yas|cinsiyet|     meslek|   sehir|aylik_gelir|       mal_mulk|\n",
      "+------+-------+---+--------+-----------+--------+-----------+---------------+\n",
      "|     1|  Cemal| 35|       E|       Isci|  ANKARA|     3500.0|          araba|\n",
      "|     2|  Ceyda| 42|       K|      Memur| KAYSERI|     4200.0|       araba|ev|\n",
      "|     3|  Timur| 30|       U|   Müzüsyen|ISTANBUL|     9000.0|araba|ev|yazlık|\n",
      "|     4|  Burcu| 29|       K|Pazarlamacı|  ANKARA|     4200.0|          araba|\n",
      "|     5|Yasemin| 23|       K|Pazarlamaci|   BURSA|     4800.0|          araba|\n",
      "+------+-------+---+--------+-----------+--------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Data cleaning\n",
    "from pyspark.sql import functions as F\n",
    "df2 = df \\\n",
    "    .withColumn(\"isim\", F.trim(F.initcap(df.isim))) \\\n",
    "    .withColumn(\"cinsiyet\", F.when(df['cinsiyet'].isNull(), \"U\").otherwise(df['cinsiyet'])) \\\n",
    "    .withColumn(\"sehir\", F.when(df['sehir'].isNull(), \"BİLİNMİYOR\").otherwise(F.trim(F.upper(df['sehir']))))\n",
    "\n",
    "#isimlerin basharflerini buyk yapip trim yapalim,\n",
    "#cinsiyet yoksa U yaz, gelmezse kendisi kalsin\n",
    "#sehirde nulla bilinmiyor yazbiliniyorsa trim ve upper uygula \n",
    "df2.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temizlmis datayi diske yazma\n",
    "df2.coalesce(1) \\\n",
    ".write \\\n",
    ".mode(\"overwrite\") \\\n",
    ".option(\"sep\",\",\") \\\n",
    ".option(\"header\",\"True\") \\\n",
    ".csv(\"/Users/resitkadir/ApacheSpark/data/simple_dirty_data2.csv\")\n",
    "\n",
    "#parca parca yazar spark\n",
    "#yaz,modu overwrite olsun,yazarken boslukla ayirsin,baslik var,ve bu path a yazsin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+--------+-----------+----------+-----------+--------------------+\n",
      "|sirano|    isim|yas|cinsiyet|     meslek|     sehir|aylik_gelir|            mal_mulk|\n",
      "+------+--------+---+--------+-----------+----------+-----------+--------------------+\n",
      "|     1|   Cemal| 35|       E|       Isci|    ANKARA|     3500.0|               araba|\n",
      "|     2|   Ceyda| 42|       K|      Memur|   KAYSERI|     4200.0|            araba|ev|\n",
      "|     3|   Timur| 30|       U|   Müzüsyen|  ISTANBUL|     9000.0|     araba|ev|yazlık|\n",
      "|     4|   Burcu| 29|       K|Pazarlamacı|    ANKARA|     4200.0|               araba|\n",
      "|     5| Yasemin| 23|       K|Pazarlamaci|     BURSA|     4800.0|               araba|\n",
      "|     6|     Ali| 33|       E|      Memur|    ANKARA|     4250.0|                  ev|\n",
      "|     7|   Dilek| 29|       K|Pazarlamaci|  ISTANBUL|     7300.0|        araba|yazlık|\n",
      "|     8|   Murat| 31|       E|   Müzüsyen|  ISTANBUL|    12000.0|araba|ev|dükkan|y...|\n",
      "|     9|   Ahmet| 33|       E|     Doktor|    ANKARA|   180000.0|     araba|ev|yazlık|\n",
      "|    10|Muhittin| 46|       E|     Berber|  ISTANBUL|    12000.0|     araba|ev|dükkan|\n",
      "|    11|Hicaziye| 47|       K| Tuhafiyeci|BİLİNMİYOR|        4.8|              dükkan|\n",
      "|    12|   Harun| 43|       E|    Tornacı|    ANKARA|     4200.0|               araba|\n",
      "|    13|   Hakkı| 33|       E|      memur|     ÇORUM|     3750.0|                  ev|\n",
      "|    14| Gülizar| 37|       K|     Doktor|     İZMIR|    14250.0|               araba|\n",
      "|    15|  Şehmuz| 41|       E|   Müzisyen|    ANKARA|     8700.0|               araba|\n",
      "+------+--------+---+--------+-----------+----------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#diske yazdigimiz file okuyalim\n",
    "df3 = spark.read \\\n",
    ".option(\"header\",\"True\") \\\n",
    ".option(\"inferSchema\",\"True\") \\\n",
    ".option(\"sep\",\",\") \\\n",
    ".csv(\"/Users/resitkadir/ApacheSpark/data/simple_dirty_data2.csv\")\n",
    "\n",
    "df3.show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6-ManualSchema\n",
    "\n",
    "**RDD ile dataframe arasindaki en buyuk fark schema dir**\n",
    "\n",
    "**Scheman bir cok katki saglar bunlardan biride optimizasyondur,kataliz optimizer adinda bir optimizor var,schema ile ve high level API ile daha iyi performans gosteriyor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    ".master(\"local[4]\") \\\n",
    ".appName(\"Csv-Üzeri-SQL\") \\\n",
    ".config(\"spark.executor.memory\",\"6g\") \\\n",
    ".config(\"spark.driver.memory\",\"2g\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    ".option(\"header\",\"True\") \\\n",
    ".option(\"inferSchema\",\"True\") \\\n",
    ".option(\"sep\",\";\") \\\n",
    ".csv(\"/Users/resitkadir/ApacheSpark/data/OnlineRetail.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|    InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|1.12.2010 08:26|     2,55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|1.12.2010 08:26|     3,39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|1.12.2010 08:26|     2,75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|1.12.2010 08:26|     3,39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|1.12.2010 08:26|     3,39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: string (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Örneğin yukarıda UnitPrice'ın  Float olmasına ihtiyaç var\n",
    "\n",
    "Spark'ın çıkarımı bize her zaman yetmez ellerimizle kendi şemamızı yapalım\n",
    "Önce gerekli kütüphaneleri içeri alalım"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, FloatType, DateType,DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Manuel \n",
    "manual_schema = StructType([\n",
    "    StructField(\"InvoiceNo\", StringType(), True),\n",
    "    StructField(\"StockCode\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"InvoiceDate\", StringType(), True),\n",
    "    StructField(\"UnitPrice\", FloatType(), True),\n",
    "    StructField(\"CustomerID\", IntegerType(), True),\n",
    "    StructField(\"Country\", StringType(), True)\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manuel schemani okuyalim\n",
    "df2 = spark.read \\\n",
    ".option(\"header\",\"True\") \\\n",
    ".option(\"sep\",\";\") \\\n",
    ".schema(manual_schema) \\\n",
    ".csv(\"/Users/resitkadir/ApacheSpark/data/OnlineRetail.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|    InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|1.12.2010 08:26|     2,55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|1.12.2010 08:26|     3,39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|1.12.2010 08:26|     2,75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|1.12.2010 08:26|     3,39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UnitPrice'da sıkıntı var. \".\" yerine \",\" olduğu için schma string'den float'a dönüştüremiyor. Çözüm bulmak gerekli.**\n",
    "\n",
    "\n",
    "### ÇÖZÜM\n",
    "Çözüm olarak veriyi okurken \",\" ile \".\" yı değiştirerek okuyalım ve dataframe'i tekrar , ve . değişmiş\n",
    "\n",
    "şekilde diske yazalım. Diskten okurken düzeltilmiş csv den okuyalım ve elle hazırlanmış şemamızı kullanalım."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = spark.read \\\n",
    ".option(\"header\",\"True\") \\\n",
    ".option(\"inferSchema\",\"True\") \\\n",
    ".option(\"sep\",\";\") \\\n",
    ".csv(\"/Users/resitkadir/ApacheSpark/data/OnlineRetail.csv\") \\\n",
    ".withColumn(\"UnitPrice\",F.regexp_replace(F.col(\"UnitPrice\"), \",\",\".\"))\n",
    "#unitprice virgul gordugun yere nokta yaz dedik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|    InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|1.12.2010 08:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|1.12.2010 08:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|1.12.2010 08:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|1.12.2010 08:26|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diske yazalim\n",
    "\n",
    "df \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"sep\",\";\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .csv(\"/Users/resitkadir/ApacheSpark/data/OnlineRetail3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yazdigimizi tekrar alalim\n",
    "\n",
    "df_temiz = spark.read \\\n",
    ".option(\"header\",\"True\") \\\n",
    ".schema(manual_schema) \\\n",
    ".option(\"sep\",\";\") \\\n",
    ".csv(\"/Users/resitkadir/ApacheSpark/data/OnlineRetail3.csv\") \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|    InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|1.12.2010 08:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|1.12.2010 08:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|1.12.2010 08:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|1.12.2010 08:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|1.12.2010 08:26|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temiz.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: float (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temiz.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-DateTimeOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    ".master(\"local[4]\") \\\n",
    ".appName(\"Csv-Üzeri-SQL\") \\\n",
    ".config(\"spark.executor.memory\",\"4g\") \\\n",
    ".config(\"spark.driver.memory\",\"2g\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    ".option(\"header\",\"True\") \\\n",
    ".option(\"inferSchema\",\"True\") \\\n",
    ".option(\"sep\",\";\") \\\n",
    ".csv(\"/Users/resitkadir/ApacheSpark/data/OnlineRetail.csv\") \\\n",
    ".select(\"InvoiceDate\").distinct()\n",
    "#ayni tarihler gelmesinn distinc ile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|    InvoiceDate|\n",
      "+---------------+\n",
      "|3.12.2010 16:50|\n",
      "|7.12.2010 12:28|\n",
      "|8.12.2010 15:02|\n",
      "+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|     InvoiceDate|\n",
      "+----------------+\n",
      "| 3.12.2010 16:50|\n",
      "| 7.12.2010 12:28|\n",
      "| 8.12.2010 15:02|\n",
      "|10.12.2010 09:53|\n",
      "|12.12.2010 13:32|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Formatı anlamak için beş satırdan gün ve ay belli olmuyor daha fazla satır görelim.\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evet şimdi anlaşıldı. Format gün.ay.yıl saat:dakika\n",
    "\n",
    "**yani dd.MM.yyyy HH:mm**\n",
    "\n",
    "Datetime da ise varsayılan format yyyy-MM-dd HH:mm:ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mevcut_format = 'dd.MM.yyyy HH:mm'\n",
    "#day, month,year,hour,minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+-------------------+\n",
      "|     InvoiceDate|normal_tarih|        standart_ts|\n",
      "+----------------+------------+-------------------+\n",
      "| 3.12.2010 16:50|  2010-12-03|2010-12-03 16:50:00|\n",
      "| 7.12.2010 12:28|  2010-12-07|2010-12-07 12:28:00|\n",
      "| 8.12.2010 15:02|  2010-12-08|2010-12-08 15:02:00|\n",
      "|10.12.2010 09:53|  2010-12-10|2010-12-10 09:53:00|\n",
      "|12.12.2010 13:32|  2010-12-12|2010-12-12 13:32:00|\n",
      "|15.12.2010 13:21|  2010-12-15|2010-12-15 13:21:00|\n",
      "|16.12.2010 08:41|  2010-12-16|2010-12-16 08:41:00|\n",
      "|17.12.2010 09:52|  2010-12-17|2010-12-17 09:52:00|\n",
      "| 9.01.2011 11:43|  2011-01-09|2011-01-09 11:43:00|\n",
      "|11.01.2011 11:38|  2011-01-11|2011-01-11 11:38:00|\n",
      "+----------------+------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df2 = df.withColumn(\"InvoiceDate\", F.trim(F.col(\"InvoiceDate\"))) \\\n",
    ".withColumn(\"normal_tarih\", F.to_date(F.col(\"InvoiceDate\"), mevcut_format)) \\\n",
    ".withColumn(\"standart_ts\", F.to_timestamp(F.col(\"InvoiceDate\"), mevcut_format)) \\\n",
    "\n",
    "#Trimledik\n",
    "#yil ay gun olsun,mevcut format ile nasil format yapmak istedgimizi yazdik\n",
    "# tarih saaat dilim yapalim\n",
    "df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+-------------------+-------------------+-------------------+----------+\n",
      "|     InvoiceDate|normal_tarih|        standart_ts|               TSTR|              TSENG| unix_time|\n",
      "+----------------+------------+-------------------+-------------------+-------------------+----------+\n",
      "| 3.12.2010 16:50|  2010-12-03|2010-12-03 16:50:00|03/12/2010 16:50:00|12-03-2010 16:50:00|1291391400|\n",
      "| 7.12.2010 12:28|  2010-12-07|2010-12-07 12:28:00|07/12/2010 12:28:00|12-07-2010 12:28:00|1291721280|\n",
      "| 8.12.2010 15:02|  2010-12-08|2010-12-08 15:02:00|08/12/2010 15:02:00|12-08-2010 15:02:00|1291816920|\n",
      "|10.12.2010 09:53|  2010-12-10|2010-12-10 09:53:00|10/12/2010 09:53:00|12-10-2010 09:53:00|1291971180|\n",
      "|12.12.2010 13:32|  2010-12-12|2010-12-12 13:32:00|12/12/2010 13:32:00|12-12-2010 13:32:00|1292157120|\n",
      "|15.12.2010 13:21|  2010-12-15|2010-12-15 13:21:00|15/12/2010 13:21:00|12-15-2010 13:21:00|1292415660|\n",
      "|16.12.2010 08:41|  2010-12-16|2010-12-16 08:41:00|16/12/2010 08:41:00|12-16-2010 08:41:00|1292485260|\n",
      "|17.12.2010 09:52|  2010-12-17|2010-12-17 09:52:00|17/12/2010 09:52:00|12-17-2010 09:52:00|1292575920|\n",
      "| 9.01.2011 11:43|  2011-01-09|2011-01-09 11:43:00|09/01/2011 11:43:00|01-09-2011 11:43:00|1294569780|\n",
      "|11.01.2011 11:38|  2011-01-11|2011-01-11 11:38:00|11/01/2011 11:38:00|01-11-2011 11:38:00|1294742280|\n",
      "+----------------+------------+-------------------+-------------------+-------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#tarih format degistirme\n",
    "format_tr = \"dd/MM/yyyy HH:mm:ss\"\n",
    "format_eng = \"MM-dd-yyyy HH:mm:ss\"\n",
    "\n",
    "df3 = df2 \\\n",
    ".withColumn(\"TSTR\", F.date_format(F.col(\"standart_ts\"), format_tr)) \\\n",
    ".withColumn(\"TSENG\", F.date_format(F.col(\"standart_ts\"), format_eng)) \\\n",
    ".withColumn(\"unix_time\", F.unix_timestamp(F.col(\"standart_ts\"))) \\\n",
    "#ustte formatlayip altta degistirebiliyoruz / - degiskligi\n",
    "df3.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+-------------------+----------+----+----+\n",
      "|     InvoiceDate|normal_tarih|        standart_ts|   bir_yil| yil|fark|\n",
      "+----------------+------------+-------------------+----------+----+----+\n",
      "| 3.12.2010 16:50|  2010-12-03|2010-12-03 16:50:00|2011-12-03|2010| 365|\n",
      "| 7.12.2010 12:28|  2010-12-07|2010-12-07 12:28:00|2011-12-07|2010| 365|\n",
      "| 8.12.2010 15:02|  2010-12-08|2010-12-08 15:02:00|2011-12-08|2010| 365|\n",
      "|10.12.2010 09:53|  2010-12-10|2010-12-10 09:53:00|2011-12-10|2010| 365|\n",
      "|12.12.2010 13:32|  2010-12-12|2010-12-12 13:32:00|2011-12-12|2010| 365|\n",
      "|15.12.2010 13:21|  2010-12-15|2010-12-15 13:21:00|2011-12-15|2010| 365|\n",
      "|16.12.2010 08:41|  2010-12-16|2010-12-16 08:41:00|2011-12-16|2010| 365|\n",
      "|17.12.2010 09:52|  2010-12-17|2010-12-17 09:52:00|2011-12-17|2010| 365|\n",
      "| 9.01.2011 11:43|  2011-01-09|2011-01-09 11:43:00|2012-01-09|2011| 365|\n",
      "|11.01.2011 11:38|  2011-01-11|2011-01-11 11:38:00|2012-01-11|2011| 365|\n",
      "+----------------+------------+-------------------+----------+----+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tarih ekleme, tarih farkı, timestamp içinden yılı alma\n",
    "df4 = df2 \\\n",
    "    .withColumn(\"bir_yil\", F.date_add(F.col(\"standart_ts\"), 365)) \\\n",
    "    .withColumn(\"yil\", F.year(F.col(\"standart_ts\"))) \\\n",
    "    .withColumn(\"fark\", F.datediff(F.col(\"bir_yil\"), F.col(\"standart_ts\")))\n",
    "\n",
    "#365 gun eklenecek standart ts e \n",
    "#yil i alalim sadece \n",
    "#farki alalim biryil-standart_ts den\n",
    "df4.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8- Pivot table\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "# Create SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "# Aşağıdaki ayarları bilgisayarınızın belleğine göre değiştirebilirsiniz\n",
    "spark = SparkSession.builder \\\n",
    ".master(\"local[4]\") \\\n",
    ".appName(\"Dataset-Olusturmak\") \\\n",
    ".config(\"spark.executor.memory\",\"4g\") \\\n",
    ".config(\"spark.driver.memory\",\"2g\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "# sparkContext'i kısaltmada tut\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tuple = ((1,0),(1,1),(0,0),(0,1),(1,0),(1,1),(0,0),(0,1),(1,1),(1,1),(0,0))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "matris_df = spark.createDataFrame(my_tuple). \\\n",
    "selectExpr(\"_1 as label\",\"_2 as prediction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|    1|         0|\n",
      "|    1|         1|\n",
      "|    0|         0|\n",
      "|    0|         1|\n",
      "|    1|         0|\n",
      "|    1|         1|\n",
      "|    0|         0|\n",
      "|    0|         1|\n",
      "|    1|         1|\n",
      "|    1|         1|\n",
      "|    0|         0|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matris_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|         0|    2|\n",
      "|    1|         1|    4|\n",
      "|    0|         1|    2|\n",
      "|    0|         0|    3|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Pivota geçmeden önce gruplanmış haline bakalım\n",
    "matris_df.groupBy(\"label\",\"prediction\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+\n",
      "|label|  0|  1|\n",
      "+-----+---+---+\n",
      "|    0|  3|  2|\n",
      "|    1|  2|  4|\n",
      "+-----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 0 yukarıda matris\n",
    "matris_df.groupBy(\"label\"). \\\n",
    "pivot(\"prediction\", [0,1]). \\\n",
    "count(). \\\n",
    "orderBy(\"label\"). \\\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+\n",
      "|label|  1|  0|\n",
      "+-----+---+---+\n",
      "|    1|  4|  2|\n",
      "|    0|  2|  3|\n",
      "+-----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 yukarıda matris\n",
    "from pyspark.sql.functions import desc\n",
    "matris_df.groupBy(\"label\"). \\\n",
    "pivot(\"prediction\", [1,0]). \\\n",
    "count(). \\\n",
    "orderBy(matris_df.label.desc()). \\\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

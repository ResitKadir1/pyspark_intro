{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Index]\n",
    "* [Create DataFrame From List](#1)\n",
    "* [Dosyadan veri okuyarak Dataframe oluşturmak](#2)\n",
    "* [WordCount](#3)\n",
    "* [Csv Dosyasindan SQL almak](#4)\n",
    "* [String Operations](#5)\n",
    "    * [Concat](#6)\n",
    "    * [Number Format](#7)\n",
    "    * [3. lower, initcap, length](#8)\n",
    "    * [4. Trim](#9)\n",
    "    * [5.Replace and Split](#10)\n",
    "    \n",
    "* [Pyspark Df to Disk](#11)\n",
    "\n",
    "* [Manual Schema](#12)\n",
    "\n",
    "* [Date Time Operations](#13)\n",
    "* [PIVOT TABLE](#14)\n",
    "* [DF to Write Kafka](#15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DataFrame From List<a class=\"anchor\" id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/Users/resitkadir/spark/spark-2.4.6/\")\n",
    "#spark session olusturuyoruz\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Aşağıdaki ayarları bilgisayarınızın belleğine göre değiştirebilirsiniz\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName(\"Create-Dataframe\") \\\n",
    "        .config(\"spark.executor.memory\",\"6g\") \\\n",
    "        .config(\"spark.driver.memory\",\"2g\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# sparkContext'i kısaltmada tut\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Row rdd yapiyoruz \n",
    "#x i al row yap\n",
    "from pyspark.sql import Row\n",
    "list_rdd = sc.parallelize([1,2,3,4,5,6,4,5]).map(lambda x: Row(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sütun ismi bir tane bile olsa Python listesi olarak parametre verilir\n",
    "#dataframe ceviriyoruz burada\n",
    "df_from_list = list_rdd.toDF(['rakamlar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|rakamlar|\n",
      "+--------+\n",
      "|       1|\n",
      "|       2|\n",
      "|       3|\n",
      "|       4|\n",
      "|       5|\n",
      "|       6|\n",
      "|       4|\n",
      "|       5|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_list.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|range|\n",
      "+-----+\n",
      "|   10|\n",
      "|   15|\n",
      "|   20|\n",
      "|   25|\n",
      "+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# range ile dataframe yaratmak\n",
    "#-1\n",
    "\n",
    "df_from_range = sc.parallelize(range(10,100,5)). \\\n",
    "                map(lambda x: (x,)). \\\n",
    "                toDF([\"range\"])\n",
    "#lambda x sonucu range basligi altina at\n",
    "df_from_range.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|   10|\n",
      "|   15|\n",
      "|   20|\n",
      "+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#yontem-2\n",
    "from pyspark.sql.types import IntegerType\n",
    "#spark session ile yapalim ()\n",
    "df_from_range2 =spark.createDataFrame(range(10,100,5), IntegerType())\n",
    "df_from_range2.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dosyadan veri okuyarak Dataframe oluşturmak <a class=\"anchor\" id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 _c0|\n",
      "+--------------------+\n",
      "|InvoiceNo;StockCo...|\n",
      "|536365;85123A;WHI...|\n",
      "|536365;71053;WHIT...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_file = spark.read.csv(\"datasets/OnlineRetail.csv\")\n",
    "df_from_file.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|      _c0|      _c1|                 _c2|     _c3|            _c4|      _c5|       _c6|           _c7|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|    InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|1.12.2010 08:26|     2,55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|1.12.2010 08:26|     3,39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_file =  spark.read. \\\n",
    "                option(\"sep\",\";\"). \\\n",
    "                csv(\"datasets/OnlineRetail.csv\")\n",
    "\n",
    "df_from_file.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_file.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|    InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|1.12.2010 08:26|     2,55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|1.12.2010 08:26|     3,39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|1.12.2010 08:26|     2,75|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_file =  spark.read. \\\n",
    "                option(\"sep\",\";\"). \\\n",
    "                option(\"header\",\"True\"). \\\n",
    "                option(\"inferSchema\",\"True\"). \\\n",
    "                csv(\"datasets/OnlineRetail.csv\")\n",
    "                #interScheman veri turleri duzelltti\n",
    "df_from_file.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: string (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print schema ,dataframe rdd farkli oldugu icin, en onemli farki bu\n",
    "df_from_file.printSchema()\n",
    "#veri turleri hepsi string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Action yapalim\n",
    "df_from_file.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|InvoiceNo|StockCode|\n",
      "+---------+---------+\n",
      "|   536365|   85123A|\n",
      "|   536365|    71053|\n",
      "|   536365|   84406B|\n",
      "|   536365|   84029G|\n",
      "|   536365|   84029E|\n",
      "|   536365|    22752|\n",
      "|   536365|    21730|\n",
      "|   536366|    22633|\n",
      "|   536366|    22632|\n",
      "|   536367|    84879|\n",
      "+---------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select ile islem yapalim\n",
    "df_from_file.select(\"InvoiceNo\",\"StockCode\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|    InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|1.12.2010 08:26|     3,39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|1.12.2010 08:26|     3,39|     17850|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|1.12.2010 08:26|     4,25|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|1.12.2010 08:26|     3,39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|1.12.2010 08:26|     2,75|     17850|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|1.12.2010 08:26|     7,65|     17850|United Kingdom|\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|1.12.2010 08:26|     2,55|     17850|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|1.12.2010 08:28|     1,85|     17850|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|1.12.2010 08:28|     1,85|     17850|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|1.12.2010 08:34|     1,69|     13047|United Kingdom|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sort\n",
    "df_from_file.sort(\"InvoiceNo\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [Quantity#107 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(Quantity#107 ASC NULLS FIRST, 200)\n",
      "   +- *(1) FileScan csv [InvoiceNo#104,StockCode#105,Description#106,Quantity#107,InvoiceDate#108,UnitPrice#109,CustomerID#110,Country#111] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/resitkadir/Desktop/pyspark_intro/Final/datasets/OnlineRetail.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<InvoiceNo:string,StockCode:string,Description:string,Quantity:int,InvoiceDate:string,UnitP...\n"
     ]
    }
   ],
   "source": [
    "df_from_file.sort(\"Quantity\").explain()\n",
    "#spark in neler yapdigihi explain ediyor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dinamik conf ayarı ve shuffle partition sayısını değiştirme\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\",\"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [Quantity#107 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(Quantity#107 ASC NULLS FIRST, 5)\n",
      "   +- *(1) FileScan csv [Description#106,Quantity#107] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/resitkadir/Desktop/pyspark_intro/Final/datasets/OnlineRetail.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Description:string,Quantity:int>\n"
     ]
    }
   ],
   "source": [
    "# Yeni conf ile sort\n",
    "df_from_file.select(\"Description\",\"Quantity\").sort(\"Quantity\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [Quantity#107 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(Quantity#107 ASC NULLS FIRST, 5)\n",
      "   +- *(1) FileScan csv [Description#106,Quantity#107] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/resitkadir/Desktop/pyspark_intro/Final/datasets/OnlineRetail.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Description:string,Quantity:int>\n"
     ]
    }
   ],
   "source": [
    "# Dinamik conf ayarı ve shuffle partition sayısını değiştirme\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\",\"5\")\n",
    "# Yeni conf ile sort\n",
    "df_from_file.select(\"Description\",\"Quantity\").sort(\"Quantity\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Count <a class=\"anchor\" id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------+\n",
      "|value                                                                                 |\n",
      "+--------------------------------------------------------------------------------------+\n",
      "|Ömer Seyfettin -         Forsa                                                        |\n",
      "|                                                                                      |\n",
      "|Akdeniz’in, kahramanlık yuvası sonsuz ufuklarına bakan küçük tepe, minimini bir çiçek |\n",
      "+--------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "# Create SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "# Aşağıdaki ayarları bilgisayarınızın belleğine göre değiştirebilirsiniz\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName(\"Dataframe-WordCount\") \\\n",
    "        .config(\"spark.executor.memory\",\"6g\") \\\n",
    "        .config(\"spark.driver.memory\",\"2g\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# sparkContext'i kısaltmada tut\n",
    "sc = spark.sparkContext\n",
    "\n",
    "veri_dosyasi = \"datasets/omer_seyfettin_forsa_hikaye.txt\"\n",
    "hikaye_df = spark.read.text(veri_dosyasi)\n",
    "#text , textfile degil\n",
    "\n",
    "hikaye_df.show(3,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RDD'de flatMap ile boşluklardan kelimeleri ayırıp aşağı atıyorduk.** \n",
    "\n",
    "**Yapısal API'de benzer işi sql explode fonksiyonu ile yapıyoruz.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|    value|\n",
      "+---------+\n",
      "|     Ömer|\n",
      "|Seyfettin|\n",
      "|        -|\n",
      "|         |\n",
      "|         |\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, split, col\n",
    "# Her bir kelimeyi boşluklarla ayıralım ve başka bir rdd'de tutalım\n",
    "kelimeler = hikaye_df.select(explode(split(col(\"value\"), \" \")).alias(\"value\"))\n",
    "#explode ile secim yapdik,icine col ile value yazdik,split ilede split fonk ile yazdik,boslukla ayir\n",
    "#alias(as demek numpy as gibi) select in icinde olmasi lazim, ve isim ver\n",
    "kelimeler.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|value|count|\n",
      "+-----+-----+\n",
      "|     |   85|\n",
      "|  bir|   32|\n",
      "|    –|   31|\n",
      "|  yıl|    8|\n",
      "| diye|    6|\n",
      "|dedi.|    5|\n",
      "| Türk|    5|\n",
      "| Kırk|    5|\n",
      "| onun|    5|\n",
      "|doğru|    5|\n",
      "+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#grouoby ile sayalim\n",
    "from pyspark.sql.functions import desc\n",
    "kelimeler.groupBy(\"value\").count().orderBy(desc(\"count\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|value|count(value)|\n",
      "+-----+------------+\n",
      "|     |          85|\n",
      "|  bir|          32|\n",
      "|    –|          31|\n",
      "|  yıl|           8|\n",
      "| diye|           6|\n",
      "| Kırk|           5|\n",
      "| Türk|           5|\n",
      "|dedi.|           5|\n",
      "| onun|           5|\n",
      "|doğru|           5|\n",
      "+-----+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ikinci yontem\n",
    "kelimeler.groupBy(\"value\").agg({\"value\": \"count\"}).orderBy(desc(\"count(value)\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CsvDosyasinaSQLAtmak <a class=\"anchor\" id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#session olustur\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName(\"Sql-Olusturmak\") \\\n",
    "        .config(\"spark.executor.memory\",\"4g\") \\\n",
    "        .config(\"spark.driver.memory\",\"2g\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "#datayi oku,dataframe olusturduk\n",
    "retailDF =  spark.read \\\n",
    "            .option(\"header\",\"True\") \\\n",
    "            .option(\"inferSchema\",\"True\") \\\n",
    "            .option(\"sep\",\";\") \\\n",
    "            .csv(\"datasets/OnlineRetail.csv\")\n",
    "\n",
    "#sanal bir tablo olusturduk\n",
    "retailDF.createOrReplaceTempView(\"tablo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InvoiceNo</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Description</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>536365</td>\n",
       "      <td>85123A</td>\n",
       "      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>\n",
       "      <td>6</td>\n",
       "      <td>1.12.2010 08:26</td>\n",
       "      <td>2,55</td>\n",
       "      <td>17850</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>536365</td>\n",
       "      <td>71053</td>\n",
       "      <td>WHITE METAL LANTERN</td>\n",
       "      <td>6</td>\n",
       "      <td>1.12.2010 08:26</td>\n",
       "      <td>3,39</td>\n",
       "      <td>17850</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>536365</td>\n",
       "      <td>84406B</td>\n",
       "      <td>CREAM CUPID HEARTS COAT HANGER</td>\n",
       "      <td>8</td>\n",
       "      <td>1.12.2010 08:26</td>\n",
       "      <td>2,75</td>\n",
       "      <td>17850</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029G</td>\n",
       "      <td>KNITTED UNION FLAG HOT WATER BOTTLE</td>\n",
       "      <td>6</td>\n",
       "      <td>1.12.2010 08:26</td>\n",
       "      <td>3,39</td>\n",
       "      <td>17850</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029E</td>\n",
       "      <td>RED WOOLLY HOTTIE WHITE HEART.</td>\n",
       "      <td>6</td>\n",
       "      <td>1.12.2010 08:26</td>\n",
       "      <td>3,39</td>\n",
       "      <td>17850</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  InvoiceNo StockCode                          Description  Quantity  \\\n",
       "0    536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6   \n",
       "1    536365     71053                  WHITE METAL LANTERN         6   \n",
       "2    536365    84406B       CREAM CUPID HEARTS COAT HANGER         8   \n",
       "3    536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6   \n",
       "4    536365    84029E       RED WOOLLY HOTTIE WHITE HEART.         6   \n",
       "\n",
       "       InvoiceDate UnitPrice  CustomerID         Country  \n",
       "0  1.12.2010 08:26      2,55       17850  United Kingdom  \n",
       "1  1.12.2010 08:26      3,39       17850  United Kingdom  \n",
       "2  1.12.2010 08:26      2,75       17850  United Kingdom  \n",
       "3  1.12.2010 08:26      3,39       17850  United Kingdom  \n",
       "4  1.12.2010 08:26      3,39       17850  United Kingdom  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retailDF.limit(5).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|    InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|1.12.2010 08:26|     2,55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|1.12.2010 08:26|     3,39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|1.12.2010 08:26|     2,75|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sql yeri actik\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "SELECT*FROM tablo\n",
    "\n",
    "\"\"\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|       Country|UnitPrice|\n",
      "+--------------+---------+\n",
      "|United Kingdom|  94911.0|\n",
      "|          EIRE|   9423.0|\n",
      "|       Germany|   7930.0|\n",
      "+--------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#groupby yapalim\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "SELECT Country ,SUM(UnitPrice) as UnitPrice\n",
    "FROM tablo\n",
    "GROUP BY Country\n",
    "ORDER BY UnitPrice DESC\n",
    "\n",
    "\"\"\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: string, CustomerID: int, Country: string]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#eger bir datasetini sik kullaniyorsak cacheleyerek daha hizli islem yapabiliriz, yontemi ise\n",
    "retailDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame String Operations <a class=\"anchor\" id=\"5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+--------+--------+-----------+-----------+---------------+\n",
      "|sirano|  isim|yas|cinsiyet|  meslek|      sehir|aylik_gelir|       mal_mulk|\n",
      "+------+------+---+--------+--------+-----------+-----------+---------------+\n",
      "|     1| Cemal| 35|       E|    Isci|     Ankara|     3500.0|          araba|\n",
      "|     2|ceyda | 42|       K|   Memur|    Kayseri|     4200.0|       araba|ev|\n",
      "|     3| Timur| 30|    null|Müzüsyen|Istanbul   |     9000.0|araba|ev|yazlık|\n",
      "+------+------+---+--------+--------+-----------+-----------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init(\"/Users/resitkadir/spark/spark-2.4.6/\")\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName(\"StringOps\") \\\n",
    "        .config(\"spark.executor.memory\",\"4g\") \\\n",
    "        .config(\"spark.driver.memory\",\"2g\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "simple_df = spark.read \\\n",
    "            .option(\"header\",\"True\") \\\n",
    "            .option(\"inferSchema\",\"True\") \\\n",
    "            .option(\"sep\",\",\") \\\n",
    "            .csv(\"datasets/simple_dirty_data.csv\")\n",
    "\n",
    "simple_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bu ders tamamen sql string ops ile ilgili o yüzden hepsini indirelim\n",
    "#from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat<a class=\"anchor\" id=\"6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---+--------+-----------+-----------+-----------+---------------+------------------------+\n",
      "|sirano|isim   |yas|cinsiyet|meslek     |sehir      |aylik_gelir|mal_mulk       |meslek_sehir            |\n",
      "+------+-------+---+--------+-----------+-----------+-----------+---------------+------------------------+\n",
      "|1     |Cemal  |35 |E       |Isci       |Ankara     |3500.0     |araba          |Isci - Ankara           |\n",
      "|2     |ceyda  |42 |K       |Memur      |Kayseri    |4200.0     |araba|ev       |Memur - Kayseri         |\n",
      "|3     |Timur  |30 |null    |Müzüsyen   |Istanbul   |9000.0     |araba|ev|yazlık|Müzüsyen - Istanbul     |\n",
      "|4     |Burcu  |29 |K       |Pazarlamacı|    Ankara |4200.0     |araba          |Pazarlamacı -     Ankara|\n",
      "|5     |Yasemin|23 |K       |Pazarlamaci|Bursa      |4800.0     |araba          |Pazarlamaci - Bursa     |\n",
      "+------+-------+---+--------+-----------+-----------+-----------+---------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Concat iki stringi birlestiren fonksiyondur\n",
    "\n",
    "# Meslek ve şehir birleştirme. Araya birşey eklemek için lit() fonksiyonu kullanırız.\n",
    "#yeni dataframe olustircaz\n",
    "#meslek ile sehri concat edelim ve yeni sutun yapalaim \n",
    "df_concat = simple_df.withColumn(\"meslek_sehir\", F.concat(col(\"meslek\"), F.lit(\" - \"), col(\"sehir\")))\n",
    "df_concat.show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number Formats <a class=\"anchor\" id=\"7\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+--------+-----------+-----------+-----------+----------------------+------------------+\n",
      "|sirano|isim    |yas|cinsiyet|meslek     |sehir      |aylik_gelir|mal_mulk              |aylik_gelir_format|\n",
      "+------+--------+---+--------+-----------+-----------+-----------+----------------------+------------------+\n",
      "|1     |Cemal   |35 |E       |Isci       |Ankara     |3500.0     |araba                 |3,500.00          |\n",
      "|2     |ceyda   |42 |K       |Memur      |Kayseri    |4200.0     |araba|ev              |4,200.00          |\n",
      "|3     |Timur   |30 |null    |Müzüsyen   |Istanbul   |9000.0     |araba|ev|yazlık       |9,000.00          |\n",
      "|4     |Burcu   |29 |K       |Pazarlamacı|    Ankara |4200.0     |araba                 |4,200.00          |\n",
      "|5     |Yasemin |23 |K       |Pazarlamaci|Bursa      |4800.0     |araba                 |4,800.00          |\n",
      "|6     | Ali    |33 |E       |Memur      |Ankara     |4250.0     |ev                    |4,250.00          |\n",
      "|7     |Dilek   |29 |K       |Pazarlamaci|Istanbul   |7300.0     |araba|yazlık          |7,300.00          |\n",
      "|8     |Murat   |31 |E       |Müzüsyen   |Istanbul   |12000.0    |araba|ev|dükkan|yazlık|12,000.00         |\n",
      "|9     |Ahmet   |33 |E       |Doktor     |Ankara     |180000.0   |araba|ev|yazlık       |180,000.00        |\n",
      "|10    |Muhittin|46 |E       |Berber     | Istanbul  |12000.0    |araba|ev|dükkan       |12,000.00         |\n",
      "+------+--------+---+--------+-----------+-----------+-----------+----------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# numara formatlama,virgulden sonra iki basamak yapalim\n",
    "#yeni sutun ismi aylik gelir format olsun\n",
    "df_number_format = simple_df \\\n",
    "                    .withColumn(\"aylik_gelir_format\", F.format_number(col(\"aylik_gelir\"), 2))\n",
    "\n",
    "df_number_format.show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. lower, initcap, length <a class=\"anchor\" id=\"8\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---+--------+-----------+-----------+-----------+---------------+------------+------------+------------+\n",
      "|sirano|isim   |yas|cinsiyet|meslek     |sehir      |aylik_gelir|mal_mulk       |meslek_lower|isim_initcap|sehir_length|\n",
      "+------+-------+---+--------+-----------+-----------+-----------+---------------+------------+------------+------------+\n",
      "|1     |Cemal  |35 |E       |Isci       |Ankara     |3500.0     |araba          |isci        |Cemal       |6           |\n",
      "|2     |ceyda  |42 |K       |Memur      |Kayseri    |4200.0     |araba|ev       |memur       |Ceyda       |7           |\n",
      "|3     |Timur  |30 |null    |Müzüsyen   |Istanbul   |9000.0     |araba|ev|yazlık|müzüsyen    |Timur       |11          |\n",
      "|4     |Burcu  |29 |K       |Pazarlamacı|    Ankara |4200.0     |araba          |pazarlamacı |Burcu       |10          |\n",
      "|5     |Yasemin|23 |K       |Pazarlamaci|Bursa      |4800.0     |araba          |pazarlamaci |Yasemin     |5           |\n",
      "+------+-------+---+--------+-----------+-----------+-----------+---------------+------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# meslek küçük harf, isimlerin ilk harfi büyük, sehirlerin kelime uzunlukları\n",
    "\n",
    "df_lower = simple_df \\\n",
    "        .withColumn(\"meslek_lower\", F.lower(col(\"meslek\"))) \\\n",
    "        .withColumn(\"isim_initcap\", F.initcap(col(\"isim\"))) \\\n",
    "        .withColumn(\"sehir_length\", F.length(col(\"sehir\")))\n",
    "\n",
    "# meslekler kucuk harfli olsun,isimlerinde(ilk harfi buyuk yaz) , ve sheir kelimesini uzunlugu nedir\n",
    "df_lower.show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4- Trim <a class=\"anchor\" id=\"9\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---+--------+-----------+-----------+-----------+---------------+-----------+-----------+----------+\n",
      "|sirano|isim   |yas|cinsiyet|meslek     |sehir      |aylik_gelir|mal_mulk       |sehir_rtrim|sehir_ltrim|sehir_trim|\n",
      "+------+-------+---+--------+-----------+-----------+-----------+---------------+-----------+-----------+----------+\n",
      "|1     |Cemal  |35 |E       |Isci       |Ankara     |3500.0     |araba          |Ankara     |Ankara     |Ankara    |\n",
      "|2     |ceyda  |42 |K       |Memur      |Kayseri    |4200.0     |araba|ev       |Kayseri    |Kayseri    |Kayseri   |\n",
      "|3     |Timur  |30 |null    |Müzüsyen   |Istanbul   |9000.0     |araba|ev|yazlık|Istanbul   |Istanbul   |Istanbul  |\n",
      "|4     |Burcu  |29 |K       |Pazarlamacı|    Ankara |4200.0     |araba          |    Ankara |Ankara     |Ankara    |\n",
      "|5     |Yasemin|23 |K       |Pazarlamaci|Bursa      |4800.0     |araba          |Bursa      |Bursa      |Bursa     |\n",
      "+------+-------+---+--------+-----------+-----------+-----------+---------------+-----------+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Trim bosluk olan yerleri bak ankara ornegi ustteki gibi(10)\n",
    "\n",
    "#rt-rightrim\n",
    "df_trim = simple_df \\\n",
    "            .withColumn(\"sehir_rtrim\", F.rtrim(col(\"sehir\"))) \\\n",
    "            .withColumn(\"sehir_ltrim\", F.ltrim(col(\"sehir\"))) \\\n",
    "            .withColumn(\"sehir_trim\", F.trim(col(\"sehir\")))\n",
    "\n",
    "df_trim.show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-Replace and Split <a class=\"anchor\" id=\"10\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---+--------+-----------+-----------+-----------+---------------+-----------+-------------------+-------------------+\n",
      "|sirano|isim   |yas|cinsiyet|meslek     |sehir      |aylik_gelir|mal_mulk       |sehir_ist  |mal_mulk_split     |mal_mulk_ilk_eleman|\n",
      "+------+-------+---+--------+-----------+-----------+-----------+---------------+-----------+-------------------+-------------------+\n",
      "|1     |Cemal  |35 |E       |Isci       |Ankara     |3500.0     |araba          |Ankara     |[araba]            |araba              |\n",
      "|2     |ceyda  |42 |K       |Memur      |Kayseri    |4200.0     |araba|ev       |Kayseri    |[araba, ev]        |araba              |\n",
      "|3     |Timur  |30 |null    |Müzüsyen   |Istanbul   |9000.0     |araba|ev|yazlık|İSTanbul   |[araba, ev, yazlık]|araba              |\n",
      "|4     |Burcu  |29 |K       |Pazarlamacı|    Ankara |4200.0     |araba          |    Ankara |[araba]            |araba              |\n",
      "|5     |Yasemin|23 |K       |Pazarlamaci|Bursa      |4800.0     |araba          |Bursa      |[araba]            |araba              |\n",
      "+------+-------+---+--------+-----------+-----------+-----------+---------------+-----------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_replace = simple_df \\\n",
    "            .withColumn(\"sehir_ist\", F.regexp_replace(col(\"sehir\"), \"Ist\", \"İST\")) \\\n",
    "            .withColumn(\"mal_mulk_split\", F.split(col(\"mal_mulk\"), \"\\\\|\")) \\\n",
    "            .withColumn(\"mal_mulk_ilk_eleman\", col(\"mal_mulk_split\")[0])\n",
    "            \n",
    "    #sehir_ist diye bir column ac ve Ist yazilan yeri IST ile degistir\n",
    "    #F split ile split ettik\n",
    "    # ve ilk elemanlari al\n",
    "df_replace.show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sirano: integer (nullable = true)\n",
      " |-- isim: string (nullable = true)\n",
      " |-- yas: integer (nullable = true)\n",
      " |-- cinsiyet: string (nullable = true)\n",
      " |-- meslek: string (nullable = true)\n",
      " |-- sehir: string (nullable = true)\n",
      " |-- aylik_gelir: double (nullable = true)\n",
      " |-- mal_mulk: string (nullable = true)\n",
      " |-- sehir_ist: string (nullable = true)\n",
      " |-- mal_mulk_split: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- mal_mulk_ilk_eleman: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_replace.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark DF to Disk (\"Write\")  <a class=\"anchor\" id=\"11\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ETL (Extract-Transform-Load) --> Cek ,donustur,yukle***\n",
    "#kirli veri(Kaynak)--->ETL --->Temiz veri (Hedef)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---+--------+-----------+-----------+-----------+---------------+\n",
      "|sirano|   isim|yas|cinsiyet|     meslek|      sehir|aylik_gelir|       mal_mulk|\n",
      "+------+-------+---+--------+-----------+-----------+-----------+---------------+\n",
      "|     1|  Cemal| 35|       E|       Isci|     Ankara|     3500.0|          araba|\n",
      "|     2| ceyda | 42|       K|      Memur|    Kayseri|     4200.0|       araba|ev|\n",
      "|     3|  Timur| 30|    null|   Müzüsyen|Istanbul   |     9000.0|araba|ev|yazlık|\n",
      "|     4| Burcu | 29|       K|Pazarlamacı|     Ankara|     4200.0|          araba|\n",
      "|     5|Yasemin| 23|       K|Pazarlamaci|      Bursa|     4800.0|          araba|\n",
      "+------+-------+---+--------+-----------+-----------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init(\"/Users/resitkadir/spark/spark-2.4.6/\")\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName(\"DFToDisk\") \\\n",
    "        .config(\"spark.executor.memory\",\"4g\") \\\n",
    "        .config(\"spark.driver.memory\",\"2g\") \\\n",
    "        .getOrCreate()\n",
    "#datayi session icine alliyoruz\n",
    "df = spark.read \\\n",
    "        .option(\"header\",\"True\") \\\n",
    "        .option(\"inferSchema\",\"True\") \\\n",
    "        .option(\"sep\",\",\") \\\n",
    "        .csv(\"datasets/simple_dirty_data.csv\")\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---+--------+-----------+--------+-----------+---------------+\n",
      "|sirano|   isim|yas|cinsiyet|     meslek|   sehir|aylik_gelir|       mal_mulk|\n",
      "+------+-------+---+--------+-----------+--------+-----------+---------------+\n",
      "|     1|  Cemal| 35|       E|       Isci|  ANKARA|     3500.0|          araba|\n",
      "|     2|  Ceyda| 42|       K|      Memur| KAYSERI|     4200.0|       araba|ev|\n",
      "|     3|  Timur| 30|       U|   Müzüsyen|ISTANBUL|     9000.0|araba|ev|yazlık|\n",
      "|     4|  Burcu| 29|       K|Pazarlamacı|  ANKARA|     4200.0|          araba|\n",
      "|     5|Yasemin| 23|       K|Pazarlamaci|   BURSA|     4800.0|          araba|\n",
      "+------+-------+---+--------+-----------+--------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Data cleaning,Session icine aldikdan sonra temizliyoruz\n",
    "from pyspark.sql import functions as F\n",
    "df2 = df \\\n",
    "    .withColumn(\"isim\", F.trim(F.initcap(df.isim))) \\\n",
    "    .withColumn(\"cinsiyet\", F.when(df['cinsiyet'].isNull(), \"U\").otherwise(df['cinsiyet'])) \\\n",
    "    .withColumn(\"sehir\", F.when(df['sehir'].isNull(), \"BİLİNMİYOR\").otherwise(F.trim(F.upper(df['sehir']))))\n",
    "\n",
    "#isimlerin basharflerini buyk yapip trim yapalim,\n",
    "#cinsiyet yoksa U yaz, gelmezse kendisi kalsin\n",
    "#sehirde nulla bilinmiyor yazbiliniyorsa trim ve upper uygula \n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temizlmis datayi diske yazma\n",
    "df2.coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"sep\",\",\") \\\n",
    "    .option(\"header\",\"True\") \\\n",
    "    .csv(\"datasets/simple_dirty_data2.csv\")\n",
    "\n",
    "#parca parca yazar spark\n",
    "#yaz,modu overwrite olsun,yazarken boslukla ayirsin,baslik var,ve bu path a yazsin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---+--------+-----------+--------+-----------+---------------+\n",
      "|sirano|   isim|yas|cinsiyet|     meslek|   sehir|aylik_gelir|       mal_mulk|\n",
      "+------+-------+---+--------+-----------+--------+-----------+---------------+\n",
      "|     1|  Cemal| 35|       E|       Isci|  ANKARA|     3500.0|          araba|\n",
      "|     2|  Ceyda| 42|       K|      Memur| KAYSERI|     4200.0|       araba|ev|\n",
      "|     3|  Timur| 30|       U|   Müzüsyen|ISTANBUL|     9000.0|araba|ev|yazlık|\n",
      "|     4|  Burcu| 29|       K|Pazarlamacı|  ANKARA|     4200.0|          araba|\n",
      "|     5|Yasemin| 23|       K|Pazarlamaci|   BURSA|     4800.0|          araba|\n",
      "+------+-------+---+--------+-----------+--------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#diske yazdigimiz file okuyalim\n",
    "df3 = spark.read \\\n",
    ".option(\"header\",\"True\") \\\n",
    ".option(\"inferSchema\",\"True\") \\\n",
    ".option(\"sep\",\",\") \\\n",
    ".csv(\"/Users/resitkadir/ApacheSpark/data/simple_dirty_data2.csv\")\n",
    "\n",
    "df3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAnual Schema <a class=\"anchor\" id=\"12\"></a>\n",
    "**RDD ile dataframe arasindaki en buyuk fark schema dir**\n",
    "\n",
    "**Scheman bir cok katki saglar bunlardan biride optimizasyondur,kataliz optimizer adinda bir optimizor var,schema ile ve high level API ile daha iyi performans gosteriyor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|    InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|1.12.2010 08:26|     2,55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|1.12.2010 08:26|     3,39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|1.12.2010 08:26|     2,75|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init(\"/Users/resitkadir/spark/spark-2.4.6/\")\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName(\"Csv-Üzeri-SQL\") \\\n",
    "        .config(\"spark.executor.memory\",\"6g\") \\\n",
    "        .config(\"spark.driver.memory\",\"2g\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\",\"True\") \\\n",
    "    .option(\"inferSchema\",\"True\") \\\n",
    "    .option(\"sep\",\";\") \\\n",
    "    .csv(\"datasets/OnlineRetail.csv\")\n",
    "\n",
    "df.show(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: string (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Örneğin yukarıda UnitPrice'ın  Float olmasına ihtiyaç var\n",
    "\n",
    "    Spark'ın çıkarımı bize her zaman yetmez ellerimizle kendi şemamızı yapalım .Önce gerekli kütüphaneleri içeri alalım"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_schema = StructType(\n",
    "[\n",
    "    StructField(\"InvoiceNo\", StringType(), True),\n",
    "    StructField(\"StockCode\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"InvoiceDate\", StringType(), True),\n",
    "    StructField(\"UnitPrice\", FloatType(), True),\n",
    "    StructField(\"CustomerID\", IntegerType(), True),\n",
    "    StructField(\"Country\", StringType(), True)\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|     null|     null|       null|    null|       null|     null|      null|   null|\n",
      "|     null|     null|       null|    null|       null|     null|      null|   null|\n",
      "|     null|     null|       null|    null|       null|     null|      null|   null|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Veriye elle hazırlanan şema ile tekrar okuma\n",
    "#manuel schemani okuyalim\n",
    "df2 = spark.read \\\n",
    "    .option(\"header\",\"True\") \\\n",
    "    .schema(manual_schema) \\\n",
    "    .option(\"sep\",\";\") \\\n",
    "    .csv(\"datasets/OnlineRetail.csv\")\n",
    "df2.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    UnitPrice'da sıkıntı var. \".\" yerine \",\" olduğu için schma string'den float'a dönüştüremiyor. Çözüm bulmak gerekli.\n",
    "   \n",
    "### ÇÖZÜM\n",
    "Çözüm olarak veriyi okurken \",\" ile \".\" yı değiştirerek okuyalım ve dataframe'i tekrar , ve . değişmiş\n",
    "\n",
    "şekilde diske yazalım. Diskten okurken düzeltilmiş csv den okuyalım ve elle hazırlanmış şemamızı kullanalım."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|    InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|1.12.2010 08:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|1.12.2010 08:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|1.12.2010 08:26|     2.75|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = spark.read \\\n",
    ".option(\"header\",\"True\") \\\n",
    ".option(\"inferSchema\",\"True\") \\\n",
    ".option(\"sep\",\";\") \\\n",
    ".csv(\"/Users/resitkadir/ApacheSpark/data/OnlineRetail.csv\") \\\n",
    ".withColumn(\"UnitPrice\",F.regexp_replace(F.col(\"UnitPrice\"), \",\",\".\"))\n",
    "\n",
    "#unitprice virgul gordugun yere nokta yaz dedik\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diske yazalim\n",
    "df \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"sep\",\";\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .csv(\"datasets\\OnlineRetail3.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yazdigimizi tekrar alalim\n",
    "\n",
    "df_temiz = spark.read \\\n",
    "    .option(\"header\",\"True\") \\\n",
    "    .schema(manual_schema) \\\n",
    "    .option(\"sep\",\";\") \\\n",
    "    .csv(\"datasets/OnlineRetail3.csv\") \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|    InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|1.12.2010 08:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|1.12.2010 08:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|1.12.2010 08:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|1.12.2010 08:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|1.12.2010 08:26|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temiz.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: float (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temiz.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date Time Operations<a class=\"anchor\" id=\"13\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|    InvoiceDate|\n",
      "+---------------+\n",
      "|3.12.2010 16:50|\n",
      "|7.12.2010 12:28|\n",
      "|8.12.2010 15:02|\n",
      "+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init(\"/Users/resitkadir/spark/spark-2.4.6/\")\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .appName(\"Csv-Üzeri-SQL\") \\\n",
    "    .config(\"spark.executor.memory\",\"4g\") \\\n",
    "    .config(\"spark.driver.memory\",\"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\",\"True\") \\\n",
    "    .option(\"inferSchema\",\"True\") \\\n",
    "    .option(\"sep\",\";\") \\\n",
    "    .csv(\"datasets/OnlineRetail.csv\") \\\n",
    "    .select(\"InvoiceDate\").distinct()\n",
    "#ayni tarihler gelmesinn distinc ile \n",
    "\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|     InvoiceDate|\n",
      "+----------------+\n",
      "| 3.12.2010 16:50|\n",
      "| 7.12.2010 12:28|\n",
      "| 8.12.2010 15:02|\n",
      "|10.12.2010 09:53|\n",
      "|12.12.2010 13:32|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Formatı anlamak için beş satırdan gün ve ay belli olmuyor daha fazla satır görelim.\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evet şimdi anlaşıldı. Format gün.ay.yıl saat:dakika\n",
    "\n",
    "**yani dd.MM.yyyy HH:mm**\n",
    "\n",
    "Datetime da ise varsayılan format yyyy-MM-dd HH:mm:ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+-------------------+\n",
      "|     InvoiceDate|normal_tarih|        standart_ts|\n",
      "+----------------+------------+-------------------+\n",
      "| 3.12.2010 16:50|  2010-12-03|2010-12-03 16:50:00|\n",
      "| 7.12.2010 12:28|  2010-12-07|2010-12-07 12:28:00|\n",
      "| 8.12.2010 15:02|  2010-12-08|2010-12-08 15:02:00|\n",
      "|10.12.2010 09:53|  2010-12-10|2010-12-10 09:53:00|\n",
      "+----------------+------------+-------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mevcut_format = 'dd.MM.yyyy HH:mm'\n",
    "#day, month,year,hour,minute\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df2 = df.withColumn(\"InvoiceDate\", F.trim(F.col(\"InvoiceDate\"))) \\\n",
    "    .withColumn(\"normal_tarih\", F.to_date(F.col(\"InvoiceDate\"), mevcut_format)) \\\n",
    "    .withColumn(\"standart_ts\", F.to_timestamp(F.col(\"InvoiceDate\"), mevcut_format)) \\\n",
    "\n",
    "#Trimledik\n",
    "#yil ay gun olsun,mevcut format ile nasil format yapmak istedgimizi yazdik\n",
    "# tarih saaat dilim yapalim\n",
    "df2.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+-------------------+-------------------+-------------------+----------+\n",
      "|     InvoiceDate|normal_tarih|        standart_ts|               TSTR|              TSENG| unix_time|\n",
      "+----------------+------------+-------------------+-------------------+-------------------+----------+\n",
      "| 3.12.2010 16:50|  2010-12-03|2010-12-03 16:50:00|03/12/2010 16:50:00|12-03-2010 16:50:00|1291391400|\n",
      "| 7.12.2010 12:28|  2010-12-07|2010-12-07 12:28:00|07/12/2010 12:28:00|12-07-2010 12:28:00|1291721280|\n",
      "| 8.12.2010 15:02|  2010-12-08|2010-12-08 15:02:00|08/12/2010 15:02:00|12-08-2010 15:02:00|1291816920|\n",
      "|10.12.2010 09:53|  2010-12-10|2010-12-10 09:53:00|10/12/2010 09:53:00|12-10-2010 09:53:00|1291971180|\n",
      "+----------------+------------+-------------------+-------------------+-------------------+----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#tarih format degistirme\n",
    "format_tr = \"dd/MM/yyyy HH:mm:ss\"\n",
    "format_eng = \"MM-dd-yyyy HH:mm:ss\"\n",
    "\n",
    "df3 = df2 \\\n",
    ".withColumn(\"TSTR\", F.date_format(F.col(\"standart_ts\"), format_tr)) \\\n",
    ".withColumn(\"TSENG\", F.date_format(F.col(\"standart_ts\"), format_eng)) \\\n",
    ".withColumn(\"unix_time\", F.unix_timestamp(F.col(\"standart_ts\"))) \\\n",
    "#ustte formatlayip altta degistirebiliyoruz / - degiskligi\n",
    "df3.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+-------------------+----------+----+----+\n",
      "|     InvoiceDate|normal_tarih|        standart_ts|   bir_yil| yil|fark|\n",
      "+----------------+------------+-------------------+----------+----+----+\n",
      "| 3.12.2010 16:50|  2010-12-03|2010-12-03 16:50:00|2011-12-03|2010| 365|\n",
      "| 7.12.2010 12:28|  2010-12-07|2010-12-07 12:28:00|2011-12-07|2010| 365|\n",
      "| 8.12.2010 15:02|  2010-12-08|2010-12-08 15:02:00|2011-12-08|2010| 365|\n",
      "|10.12.2010 09:53|  2010-12-10|2010-12-10 09:53:00|2011-12-10|2010| 365|\n",
      "+----------------+------------+-------------------+----------+----+----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tarih ekleme, tarih farkı, timestamp içinden yılı alma\n",
    "df4 = df2 \\\n",
    "    .withColumn(\"bir_yil\", F.date_add(F.col(\"standart_ts\"), 365)) \\\n",
    "    .withColumn(\"yil\", F.year(F.col(\"standart_ts\"))) \\\n",
    "    .withColumn(\"fark\", F.datediff(F.col(\"bir_yil\"), F.col(\"standart_ts\")))\n",
    "\n",
    "#365 gun eklenecek standart ts e \n",
    "#yil i alalim sadece \n",
    "#farki alalim biryil-standart_ts den\n",
    "df4.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pivot table<a class=\"anchor\" id=\"14\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|    1|         0|\n",
      "|    1|         1|\n",
      "|    0|         0|\n",
      "|    0|         1|\n",
      "|    1|         0|\n",
      "|    1|         1|\n",
      "|    0|         0|\n",
      "|    0|         1|\n",
      "|    1|         1|\n",
      "|    1|         1|\n",
      "|    0|         0|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "import findspark\n",
    "findspark.init(\"/Users/resitkadir/spark/spark-2.4.6/\")\n",
    "# Create SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "# Aşağıdaki ayarları bilgisayarınızın belleğine göre değiştirebilirsiniz\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .appName(\"Dataset-Olusturmak\") \\\n",
    "    .config(\"spark.executor.memory\",\"4g\") \\\n",
    "    .config(\"spark.driver.memory\",\"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# sparkContext'i kısaltmada tut\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "my_tuple = ((1,0),(1,1),(0,0),(0,1),(1,0),(1,1),(0,0),(0,1),(1,1),(1,1),(0,0))\n",
    "                    \n",
    "matris_df = spark.createDataFrame(my_tuple). \\\n",
    "selectExpr(\"_1 as label\",\"_2 as prediction\")\n",
    "matris_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|         0|    2|\n",
      "|    1|         1|    4|\n",
      "|    0|         1|    2|\n",
      "|    0|         0|    3|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Pivota geçmeden önce gruplanmış haline bakalım\n",
    "matris_df.groupBy(\"label\",\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+\n",
      "|label|  0|  1|\n",
      "+-----+---+---+\n",
      "|    0|  3|  2|\n",
      "|    1|  2|  4|\n",
      "+-----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 0 yukarıda matris\n",
    "matris_df.groupBy(\"label\"). \\\n",
    "        pivot(\"prediction\", [0,1]). \\\n",
    "        count(). \\\n",
    "        orderBy(\"label\"). \\\n",
    "        show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+\n",
      "|label|  1|  0|\n",
      "+-----+---+---+\n",
      "|    1|  4|  2|\n",
      "|    0|  2|  3|\n",
      "+-----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 yukarıda matris\n",
    "from pyspark.sql.functions import desc\n",
    "matris_df.groupBy(\"label\"). \\\n",
    "pivot(\"prediction\", [1,0]). \\\n",
    "count(). \\\n",
    "orderBy(matris_df.label.desc()). \\\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To write KAFKA <a class=\"anchor\" id=\"15\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+---------+-----+\n",
      "|_c0|   TV|Radio|Newspaper|Sales|\n",
      "+---+-----+-----+---------+-----+\n",
      "|  1|230.1| 37.8|     69.2| 22.1|\n",
      "|  2| 44.5| 39.3|     45.1| 10.4|\n",
      "+---+-----+-----+---------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init(\"/Users/resitkadir/spark/spark-2.4.6/\")\n",
    "\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell'\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName(\"WriteToKafka\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", True) \\\n",
    "        .load(\"datasets/Advertising.csv\")\n",
    "\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+-----+---+\n",
      "|   TV|Radio|Newspaper|Sales|key|\n",
      "+-----+-----+---------+-----+---+\n",
      "|230.1| 37.8|     69.2| 22.1|  1|\n",
      "| 44.5| 39.3|     45.1| 10.4|  2|\n",
      "+-----+-----+---------+-----+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumn(\"key\", col('_c0')).drop('_c0')\n",
    "df2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|key|               value|\n",
      "+---+--------------------+\n",
      "|  1|230.1,37.8,69.2,22.1|\n",
      "|  2| 44.5,39.3,45.1,10.4|\n",
      "+---+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df2.select('key',\n",
    "                 concat(\n",
    "                 col('TV'), lit(\",\"),\n",
    "                 col('Radio'), lit(\",\"),\n",
    "                col('Newspaper'), lit(\",\"),\n",
    "                col('Sales')\n",
    "                 ).alias(\"value\")\n",
    "\n",
    ")\n",
    "\n",
    "df3.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Failed to find data source: kafka. Please deploy the application as per the deployment section of \"Structured Streaming + Kafka Integration Guide\".;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/spark/spark-2.4.6/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-2.4.6/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o542.save.\n: org.apache.spark.sql.AnalysisException: Failed to find data source: kafka. Please deploy the application as per the deployment section of \"Structured Streaming + Kafka Integration Guide\".;\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:652)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:246)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-f6724cc507f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kafka\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kafka.bootstrap.servers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"localhost:9092\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"topic\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-2.4.6/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-2.4.6/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-2.4.6/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Failed to find data source: kafka. Please deploy the application as per the deployment section of \"Structured Streaming + Kafka Integration Guide\".;'"
     ]
    }
   ],
   "source": [
    "df3 \\\n",
    "    .write \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"topic\", \"test\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://spark.apache.org/docs/2.4.0/structured-streaming-programming-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sonuclar icin kafka consumer i calistirmamiz lazim\n",
    "#sonuc calismadi ---(92)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

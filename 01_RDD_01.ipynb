{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [INDEX]\n",
    "\n",
    "* [Spark Creations]\n",
    "    * [SparkContext oluşturma yöntem-1: SparkSession](#1)\n",
    "    * [SparkContext oluşturma yöntem-2: ParkSession ve SparkConf](#2)\n",
    "    * [SparkContext oluşturma yöntem-3: SparkContext ve SparkConf](#3)\n",
    "    * [Python listelerinden RDD oluşturmak](#4)\n",
    "    * [Python sözlükten (dictionary) RDD oluşturmak](#5)\n",
    "    * [Metin dosyalarından RDD oluşturmak](#6)\n",
    "* [Basic Transormations and Actions](#7)\n",
    "    * [RDD transformations](#8)\n",
    "    * [2 RDD transformations](#9)\n",
    "    * [Basic Actions on one RDD](#10)\n",
    "* [MAP vs FLATMAP](#11)\n",
    "* [TEST ON MAP vs FLAtMAP](#12)\n",
    "* [MAP vs FLATMAP Functions](#13)\n",
    "* [RDD FILTER tansformation](#14)\n",
    "* [RRD-JOIN](#15)\n",
    "* [PAIR RDD Operations](#16)\n",
    "* [Excel-Dataframe-RDD](#17)\n",
    "* [BroadcastVariablesOps](#18)\n",
    "* [RDD_Wordcount](#19)\n",
    "* [AND Others](#20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RDD parallelize demekdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(findspark.init(\"/Users/resitkadir/spark/spark-2.4.6/\"))\n",
    "\n",
    "# Create SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  SparkContext oluşturma yöntem-1: SparkSession <a class=\"anchor\" id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aşağıdaki ayarları bilgisayarınızın belleğine göre değiştirebilirsiniz\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName(\"RDD-1\") \\\n",
    "        .config(\"spark.executor.memory\",\"4g\") \\\n",
    "        .config(\"spark.driver.memory\",\"2g\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# sparkContext'i kısaltmada tut\n",
    "sc = spark.sparkContext\n",
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkContext oluşturma yöntem-2: ParkSession ve SparkConf  <a class=\"anchor\" id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf =   SparkConf() \\\n",
    "        .setMaster(\"local[4]\") \\\n",
    "        .setAppName(\"RDD-Olusturmak-2\") \\\n",
    "        .setExecutorEnv(\"spark.executor.memory\",\"4g\") \\\n",
    "        .setExecutorEnv(\"spark.driver.memory\",\"4g\")\n",
    "\n",
    "pyspark = SparkSession.builder \\\n",
    "        .config(conf=conf) \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = pyspark.sparkContext\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkContext oluşturma yöntem-3: SparkContext ve SparkConf <a class=\"anchor\" id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkConf = SparkConf() \\\n",
    "            .setMaster(\"local[4]\") \\\n",
    "            .setAppName(\"RDD-Olusturmak-3\") \\\n",
    "            .setExecutorEnv(\"spark.executor.memory\",\"2g\") \\\n",
    "            .setExecutorEnv(\"spark.driver.memory\",\"1g\")\n",
    "\n",
    "\n",
    "\n",
    "sc = SparkContext(conf=sparkConf)\n",
    "\n",
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python listelerinden RDD oluşturmak <a class=\"anchor\" id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ahmet', 25), ('Cemal', 29)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([('Ahmet',25),('Cemal',29),('İnci',38),('Burcu',33)])\n",
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Ahmet', 25], ['Cemal', 29], ['İnci', 38]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = sc.parallelize([['Ahmet',25],['Cemal',29],['İnci',38],['Burcu',33]])\n",
    "rdd2.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count\n",
    "rdd2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [4, 5, 6]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sayilarRDD = sc.parallelize([[1,2,3],[4,5,6]])\n",
    "sayilarRDD.take(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python sözlükten (dictionary) RDD oluşturmak <a class=\"anchor\" id=\"5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ogrenci': ['Ali', 'Mehmet', 'Ayse'], 'Notlar': [70, 80, 90]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sözlük oluşturma\n",
    "my_dict ={\n",
    "    \"Ogrenci\":['Ali','Mehmet','Ayse'],\n",
    "    \"Notlar\":[70,80,90]}\n",
    "my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ogrenci</th>\n",
       "      <th>Notlar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ali</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mehmet</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ayse</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Ogrenci  Notlar\n",
       "0     Ali      70\n",
       "1  Mehmet      80\n",
       "2    Ayse      90"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dict to frame donusumu yaoiyoruz\n",
    "import pandas as pd\n",
    "pdDF = pd.DataFrame(my_dict)\n",
    "pdDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf =   SparkConf() \\\n",
    "        .setMaster(\"local[4]\") \\\n",
    "        .setAppName(\"RDD-Olusturmak-2\") \\\n",
    "        .setExecutorEnv(\"spark.executor.memory\",\"4g\") \\\n",
    "        .setExecutorEnv(\"spark.driver.memory\",\"4g\")\n",
    "\n",
    "pyspark = SparkSession.builder \\\n",
    "        .config(conf=conf) \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = pyspark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|Ogrenci|Notlar|\n",
      "+-------+------+\n",
      "|    Ali|    70|\n",
      "| Mehmet|    80|\n",
      "|   Ayse|    90|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd_from_pandasDF = pyspark.createDataFrame(pdDF)\n",
    "rdd_from_pandasDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Ogrenci='Ali', Notlar=70),\n",
       " Row(Ogrenci='Mehmet', Notlar=80),\n",
       " Row(Ogrenci='Ayse', Notlar=90)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get into pandas\n",
    "rdd_from_pandas = rdd_from_pandasDF.rdd\n",
    "rdd_from_pandas.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metin dosyalarından RDD oluşturmak <a class=\"anchor\" id=\"6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['InvoiceNo;StockCode;Description;Quantity;InvoiceDate;UnitPrice;CustomerID;Country',\n",
       " '536365;85123A;WHITE HANGING HEART T-LIGHT HOLDER;6;1.12.2010 08:26;2,55;17850;United Kingdom']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_metin = sc.textFile(\"datasets/OnlineRetail.csv\")\n",
    "rdd_metin.take(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Basic Tranformations and Actions <a class=\"anchor\" id=\"7\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import findspark\n",
    "findspark.init(\"/Users/resitkadir/spark/spark-3.0.0/\")\n",
    "import pyspark \n",
    "\n",
    "# Create SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .appName(\"RDD-Olusturmak\") \\\n",
    "    .config(\"spark.executor.memory\",\"4g\") \\\n",
    "    .config(\"spark.driver.memory\",\"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 RDD transformations**<a class=\"anchor\" id=\"8\"></a>\n",
    "\n",
    "<font color='green'>**map()** </font>\n",
    "\n",
    "<font color='green'>**filter()** </font>\n",
    "\n",
    "<font color='green'>**flatMap()**  </font>\n",
    " \"Map in yapdigi isi her bir element icin ayri ayri yapar\"\n",
    "                                                                        \n",
    "<font color='green'>**distinct()  :**  </font>*duplicate(unique yapar) valuelari tek yazar alir*\n",
    "                \n",
    "<font color='green'>**sample()**  </font>\n",
    "\n",
    "\n",
    "![exa](IMG/RDD_1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>**reduceByKey()** : </font> *Anahtar icin degerleri birlestirir.Her bir anahtara ait degerlerin toplamini iceren bir RDD doner*\n",
    "     \n",
    "   **her anahtar icin degereri topladi**\n",
    "   \n",
    "     1 icin 2 ve 3 icin 4+6=10\n",
    " \n",
    "    currentRDD{(1,2),(3,4),(3,6)}->**reduceByKey((x,y)=> x+y)->newRDD{(1,2),(3,10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 10)]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#anahtar bazinda reduce et\n",
    "rdd = sc.parallelize([(1,2),(3,4),(3,6)])\n",
    "rdd.reduceByKey(lambda x,y :x+y).take(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>**groupByKey()** : </font> *anahtarlari grupla*\n",
    "    \n",
    "       currentRDD{(1,2),(3,4),(3,6)}->**groupByKey() -> newRDD{(1,(2)),(3,(4,6))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, <pyspark.resultiterable.ResultIterable at 0x7f9604dab810>),\n",
       " (3, <pyspark.resultiterable.ResultIterable at 0x7f9604dab550>)]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#anahtar bazinda gruplama\n",
    "rdd.groupByKey().take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>**mapValues()** : </font> *PairRDD nin degerlerine belirtilen fonksiyonu uygular,ornegin degerleri 100 ile carp*\n",
    "    \n",
    "       currentRDD{(1,2),(3,4),(3,6)}->**rdd1.mapValues(x => x*100) -> newRDD{(1,200),(3,400),(3,600)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 200), (3, 400), (3, 600)]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.mapValues(lambda x :x*100).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>**keys()** : </font>*pairRdd nin anahtarlarini iceren bir RDD doner*\n",
    "\n",
    "       currentRDD{(1,2),(3,4),(3,6)}->**rdd1.keys() -> newRDD{(1,3,3)}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 3]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.keys().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>**values()** : </font>*pairRdd nin degerleri iceren bir RDD doner*\n",
    "\n",
    "       currentRDD{(1,2),(3,4),(3,6)}->**rdd1.values() -> newRDD{(2,4,6)}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.values().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>**sortByKey()** : </font>*anahtara gore siralanmis bir RDD doner*\n",
    "\n",
    "       currentRDD{(1,2),(3,4),(3,6)}->**rdd1.sortByKey() -> newRDD{(1,2),(3,4),(3,6)}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 4), (3, 6)]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**take**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_ = [x for x in range(10)]\n",
    "list_\n",
    "liste_rdd =sc.parallelize(list_)\n",
    "liste_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MAP\n",
    "liste_rdd.map(lambda x : x**2 ).take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filter\n",
    "liste_rdd.filter(lambda x : x < 5 ).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He is really cool', 'she is at home.', \"it doesn't say so much\"]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ = [\"He is really cool\",\"she is at home.\",\"it doesn't say so much\"]\n",
    "#Create RDD with parallize\n",
    "text_rdd = sc.parallelize(text_)\n",
    "text_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HE IS REALLY COOL', 'SHE IS AT HOME.', \"IT DOESN'T SAY SO MUCH\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_rdd.map(lambda x : x.upper()).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H', 'E', ' ']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#flatmap,harf olarak verdi\n",
    "text_rdd.flatMap(lambda x : x.upper()).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HE', 'IS', 'REALLY', 'COOL']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##word to word splitin\n",
    "text_rdd.flatMap(lambda x : x.split(\" \")).map(lambda x : x.upper()).take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 1, 5, 2, 6]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DISTINC,unique ler sadece kalir\n",
    "list_2 = [1,1,2,2,4,5,6,6]\n",
    "list_2_rdd = sc.parallelize(list_2)\n",
    "list_2_rdd.distinct().take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 2, 2, 4, 6, 6]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0.7 %70 al gibi\n",
    "#42 verilen orneklar ayni kalsin\n",
    "list_2_rdd.sample(True,0.7,42).take(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 RDD transformations**<a class=\"anchor\" id=\"9\"></a>\n",
    "![exa](IMG/RDD_2.png)\n",
    "<font color='green'>**union()** : </font> \"iki RDD nin elemanlarini birlestirir tek bir RDD doner \"\n",
    "\n",
    "    --> rdd1 = {1,2,9,4,5,36}\n",
    "\n",
    "    -->rdd2 = {1,4,9,16,25,36}\n",
    "\n",
    "    rdd1.union(rdd2) -> rddUnion {1,2,9,4,5,36,1,4,9,16,25,36}\n",
    "    \n",
    "<font color='green'>**intersection()**  : </font>\n",
    " \"kesisim ikisine ait ortak elemanlari alir\"\n",
    "\n",
    "    rdd1.intersection(rdd2)->rddIntersection{1,4,9,36}\n",
    "\n",
    "<font color='green'>**subtract()**   : </font>\n",
    "  \"ilkyazilan (rdd1)deki unique degerleri sadece al\"\n",
    "    \n",
    "     rdd1.subtract(rdd2)-->rddSubtract{2,5}\n",
    "     \n",
    "     rdd2.subtract(rdd1)-->rddSubtract{16,25}\n",
    "\n",
    "<font color='green'>**Cartesian()**  : </font> \"\"butun olasilikdaki couple lari alir\"\n",
    "\n",
    "    rdd1.cartesian(rdd2)-->{(1,4),(1,9),.....(36,1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>**subtractByKey()** : </font>**diger anahtara ait elemanlari cikarir**\n",
    "\n",
    "    *3 anahtari olan sayilari at ve olmayan kalsin*\n",
    "\n",
    "    rdd1->{(1,2),(3,4),(3,6)}\n",
    "    \n",
    "    rdd2->{(3,9)}\n",
    "    \n",
    "    rdd1.subtractByKey(rdd2)-->{(1,2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 4), (3, 6)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(1,2),(3,4),(3,6)])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 9]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = sc.parallelize([3,9])\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 22.0 failed 1 times, most recent failure: Lost task 7.0 in stage 22.0 (TID 71, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/resitkadir/spark/spark-2.4.6/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/Users/resitkadir/spark/spark-2.4.6/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/resitkadir/spark/spark-2.4.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/resitkadir/spark/spark-2.4.6/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/Users/resitkadir/spark/spark-2.4.6/python/pyspark/rdd.py\", line 1983, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\nTypeError: 'int' object is not subscriptable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor59.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/resitkadir/spark/spark-2.4.6/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/Users/resitkadir/spark/spark-2.4.6/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/resitkadir/spark/spark-2.4.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/resitkadir/spark/spark-2.4.6/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/Users/resitkadir/spark/spark-2.4.6/python/pyspark/rdd.py\", line 1983, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\nTypeError: 'int' object is not subscriptable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-2e97a4d84956>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtractByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark/spark-2.4.6/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-2.4.6/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-2.4.6/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-2.4.6/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 22.0 failed 1 times, most recent failure: Lost task 7.0 in stage 22.0 (TID 71, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/resitkadir/spark/spark-2.4.6/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/Users/resitkadir/spark/spark-2.4.6/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/resitkadir/spark/spark-2.4.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/resitkadir/spark/spark-2.4.6/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/Users/resitkadir/spark/spark-2.4.6/python/pyspark/rdd.py\", line 1983, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\nTypeError: 'int' object is not subscriptable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor59.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/resitkadir/spark/spark-2.4.6/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/Users/resitkadir/spark/spark-2.4.6/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/resitkadir/spark/spark-2.4.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/resitkadir/spark/spark-2.4.6/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/Users/resitkadir/spark/spark-2.4.6/python/pyspark/rdd.py\", line 1983, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\nTypeError: 'int' object is not subscriptable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\n"
     ]
    }
   ],
   "source": [
    "rdd.subtractByKey(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess as sp\n",
    "sp.call(\"cls\",shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (1, 4), (1, 9)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([1,2,9,4,5,36])\n",
    "rdd2 = sc.parallelize([1,4,9,16,25,36])\n",
    "#union\n",
    "rdd1.union(rdd2).take(12) #[1, 2, 9, 4, 5, 36, 1, 4, 9, 16, 25, 36]\n",
    "#intersection\n",
    "rdd1.intersection(rdd2).take(6) #[1, 9, 4, 36]\n",
    "#subtract\n",
    "rdd1.subtract(rdd2).take(5) #[2, 5]\n",
    "rdd2.subtract(rdd1).take(5) # [16, 25]\n",
    "#Cartesian\n",
    "rdd1.cartesian(rdd2).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic Actions on one RDD (uygulama yerleri)**<a class=\"anchor\" id=\"10\"></a>\n",
    "\n",
    "<font color='green'> **collect()** : </font> *RDD uzerindeki tum elemanlari driver pc uzerine doner.Buyul RDD lerde calistirmak tehlikelidir,ornegin bizim driver 4 gb olsun ama veri 500gb olursa sikinti buyur*\n",
    "\n",
    "    --> rdd1 = {1,2,9,4,5,36}\n",
    "    Sonucu liste olarak verir\n",
    "    rdd1.collect() --> [1,2,9,4,5,36]\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "<font color='green'>**count() :** </font>*Eleman sayisini sayar*\n",
    "\n",
    "    rdd1.count()--> Long=6\n",
    "\n",
    "<font color='green'>**countByValue() :** </font>*Her bir elemanin RDD icinde kac kez tekrarlandigini hesaplar ve bir tuple doner*\n",
    "\n",
    "**ilk elemani sayiyi ve sonra Her elemandan kac tane oldugunu gosterir**\n",
    "      \n",
    "      \n",
    "     rdd1.countByValue() --> Map[Int,Long] = Map(5->1,1->1,....,36->1)\n",
    "\n",
    "<font color='green'>**take() :** </font> *RDD icinde istenen sayida eleman doner*\n",
    "\n",
    "    rdd1.take(3)-->Array[int] = Array(1,2,5)\n",
    "\n",
    "<font color='green'>**top() :** </font>*En ustteki 3 degeri al getir(buyukluk)*\n",
    "\n",
    "\n",
    "    rdd1.top(3)-->Array[Int]=Array(36,9,5)\n",
    "\n",
    "<font color='green'>**union()** : </font>\n",
    "\n",
    "    \n",
    "\n",
    "<font color='green'>**takeOrdered() :** </font>\n",
    "\n",
    "*RDD icindeki elemanlari siralayarak belirtilen sayi kadarini bir array olarak doner*\n",
    "\n",
    "    rdd1.takeOrdered(6)--->Array[Int]-->Array(1,2,4,5,9,36)\n",
    "\n",
    "<font color='green'>**takeSample() :** </font>*RDD icinden istenen mikatrda orneklem iceren bir Array doner*\n",
    "\n",
    "**parametreler:**\n",
    "\n",
    "**withReplacement :** Boolean(Aldigini yerine koyalimmi?) , \n",
    "\n",
    "**num:Int**(kac tane eleman alicaz) ,\n",
    "\n",
    "**seed:Long**(Her tekrarlandiginda ayni elemanlar gelsin mi?orenkde 42 verdikce 9,2,5 gelir yoksa degisir)\n",
    "\n",
    "    rdd1.takeSample(false,3,42)-->Array[Int] = Array(9,2,5)\n",
    "\n",
    "<font color='green'>**reduce() :** </font>*RDD uzerindeki elemanlari paralele olarak uygulayarak bir sonuc uretir.Orenign toplama*\n",
    "\n",
    "    (1,2,4,5,9,36)\n",
    "    \n",
    "    rdd1.reduce((x,y) => x+y) -->57\n",
    "\n",
    "<font color='green'>**fold() :** </font>*Reduce ile aynidir sadece sifir degeri farki vardir*\n",
    "\n",
    "**fold(zero)(func)**\n",
    "\n",
    "    rdd1.fold(0)((x,y) => x+y)-->57\n",
    "\n",
    "<font color='green'>**aggregate() :** </font>** *Her bir pattition elemanlarini kumeleme(aggregation) fonskiyonunu uygular ve combine fonksiyonlari ile bu sonuclari birlestirir*\n",
    "\n",
    "**aggregate(zero)(seqOp,combOp)**\n",
    "\n",
    "    rdd1.aggregate((0,0))\n",
    "    \n",
    "    ((x,y) => (x._+y,x._2+1),\n",
    "               (x._1+y._1,x._2+y._2))  ->(int,int)=(57,6)\n",
    "               \n",
    "<font color='green'>**foreach()  :** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,9,4,2,4,5,1,1,7])\n",
    "#Count\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {1: 3, 2: 2, 9: 1, 4: 2, 5: 1, 7: 1})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CountByValue,hangi sayidan kac tane var\n",
    "rdd.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 9]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take\n",
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 7, 5, 4, 4]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top,sirala\n",
    "rdd.top(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 2]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#takeordered,takein siralanmis hali\n",
    "rdd.takeOrdered(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 9]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take sample,5 deger al ,seed 33 sectik,\n",
    "rdd.takeSample(\"false\",5,33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 9]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.takeSample(\"false\",5,33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 9]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.takeSample(\"True\",5,33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reduce\n",
    "rdd.reduce(lambda x,y :x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fold\n",
    "rdd.fold(0,lambda x,y :x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1,2,3,9,4,10,5,36,8]\n",
    "\n",
    "**sc.parallelize ile datayi dagittik**\n",
    "\n",
    "*ilk uc elemanimiz thread-1 gitti(1,2,3),*\n",
    "\n",
    "0 degeri <font color='green'>**x[0]** </font>ataniyor ve oyle basliyor ve y 1 ile basliyor thread ikide x[0] yine 0 ile basliyor y ise 9 ile\n",
    "\n",
    "<font color='green'>**x[1]** </font> ile isede yine 0 dan baslayip kac tane eleman oldugunu sayiyor\n",
    "\n",
    "*ve thread-2 ye (9,4,10) gitti*\n",
    "\n",
    "![Aggregate](IMG/Aggregate.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78, 9)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#aggregate\n",
    "rdd_a = [1,2,3,9,4,10,5,36,8]\n",
    "rdd_a = sc.parallelize(rdd_a)\n",
    "rdd_a.aggregate((0,0),(lambda x,y : (x[0] + y , x[1]+1)),(lambda x , y : (x[0] + y[0],x[1]+y[1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MAP vs FLARMAP**<a class=\"anchor\" id=\"11\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sirano,isim,yas,meslek,sehir,aylik_gelir',\n",
       " '1,Cemal,35,Isci,Ankara,3500',\n",
       " '2,Ceyda,42,Memur,Kayseri,4200',\n",
       " '3,Timur,30,Müzisyen,Istanbul,9000',\n",
       " '4,Burcu,29,Pazarlamaci,Ankara,4200']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init(findspark.init(\"/Users/resitkadir/spark/spark-3.0.0/\"))\n",
    "from pyspark import SparkContext\n",
    "#\n",
    "sc = SparkContext(\"local[4]\",\"map_flat_Map\")\n",
    "#\n",
    "ppl_RDD= sc.textFile( \"datasets/simple_data.csv\")\n",
    "#\n",
    "ppl_RDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,Cemal,35,Isci,Ankara,3500',\n",
       " '2,Ceyda,42,Memur,Kayseri,4200',\n",
       " '3,Timur,30,Müzisyen,Istanbul,9000',\n",
       " '4,Burcu,29,Pazarlamaci,Ankara,4200',\n",
       " '5,Yasemin,23,Pazarlamaci,Bursa,4800']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppl_RDD_ = ppl_RDD.filter(lambda x : \"sirano\" not in x)\n",
    "#sirano satirini alma\n",
    "ppl_RDD_.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,CEMAL,35,ISCI,ANKARA,3500',\n",
       " '2,CEYDA,42,MEMUR,KAYSERI,4200',\n",
       " '3,TIMUR,30,MÜZISYEN,ISTANBUL,9000',\n",
       " '4,BURCU,29,PAZARLAMACI,ANKARA,4200',\n",
       " '5,YASEMIN,23,PAZARLAMACI,BURSA,4800']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#map satira odaklanir\n",
    "ppl_RDD_.map(lambda x : x.upper()).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', ',', 'C', 'E', 'M']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#flatmap,harf'e\n",
    "ppl_RDD_.flatMap(lambda x :x.upper()).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " 'CEMAL',\n",
       " '35',\n",
       " 'ISCI',\n",
       " 'ANKARA',\n",
       " '3500',\n",
       " '2',\n",
       " 'CEYDA',\n",
       " '42',\n",
       " 'MEMUR',\n",
       " 'KAYSERI',\n",
       " '4200',\n",
       " '3',\n",
       " 'TIMUR',\n",
       " '30']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppl_RDD_.flatMap(lambda x :x.split(\",\")).map(lambda x :x.upper()).take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEST ON MAP vs FLAtMAP**<a class=\"anchor\" id=\"12\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. SparkContext sınıfını kullanarak local modda çalışan 2 çekirdek, 2 Gb. driver, 3 Gb executor belleğine sahip, \"Test\" isimli ekrana \"Merhaba Spark\" yazan bir Spark uygulaması yazınız**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(findspark.init(\"/Users/resitkadir/spark/spark-3.0.0/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark Context sinifi ile kurmamiz lazim\n",
    "from pyspark import SparkContext\n",
    "from pyspark.conf import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_conf =   SparkConf(). \\\n",
    "                  setMaster(\"local[2]\"). \\\n",
    "                setAppName(\"Test\"). \\\n",
    "                set(\"spark.driver.memory\",\"2g\"). \\\n",
    "                setExecutorEnv(\"spark.executor.memory\",\"3g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello Spark\n"
     ]
    }
   ],
   "source": [
    "#spark context olusturalim\n",
    "sc = SparkContext(conf=spark_conf)\n",
    "print(\"hello Spark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.) 3,7,13,15,22,36,7,11,3,25 rakamlarından bir RDD oluşturunuz.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 7, 13, 15, 22, 36, 7, 11, 3, 25]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_=[3,7,13,15,22,36,7,11,3,25]\n",
    "rdd_s =sc.parallelize(rdd_)\n",
    "rdd_s.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. \"Spark'ı öğrenmek çok heyecan verici\" cümlesinin tüm harflerini büyük harf yapınız.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"SPARK'I ÖĞRENMEK ÇOK HEYECAN VERICI\"]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [\"Spark'ı öğrenmek çok heyecan verici\"]\n",
    "a=sc.parallelize(a)\n",
    "a.map(lambda x : x.upper()).take(4)\n",
    "\n",
    "#yol-2\n",
    "text_rdd = sc.parallelize([\"Spark'ı öğrenmek çok heyecan verici\"])\n",
    "text_rdd.map(lambda x :x.upper()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.) https://github.com/veribilimiokulu/udemy-apache-spark/blob/master/docs/Ubuntu_Spark_Kurulumu.txt adresindeki text dosyasını Spark ile okuyarak kaç satırdan oluştuğunu ekrana yazdırınız.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ = sc.textFile(\"datasets/soru_4.txt\").count()\n",
    "text_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. https://github.com/veribilimiokulu/udemy-apache-spark/blob/master/docs/Ubuntu_Spark_Kurulumu.txt adresindeki text dosyasını Spark ile okuyarak kaç kelimeden oluştuğunu ekrana yazdırınız. (Kelimeler tekrarlanabilir)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ = sc.textFile(\"datasets/soru_4.txt\")\n",
    "text_.flatMap(lambda x : x.split(\" \")).map(lambda x : x.upper()).count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. İkinci sorudaki rakam listesi ile 1,2,3,4,5,6,7,8,9,10 listesi arasındaki kesişim kümesini(ortak rakamları) Spark uygulaması ile ekrana yazdırınız.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 7]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_s.collect()\n",
    "rdd_2 = sc.parallelize([1,2,3,4,5,6,7,8,9,10 ])\n",
    "rdd_s.intersection(rdd_2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. İkinci sorudaki rakamların tekil (rakamların tekrarlanmaması) halinden oluşan bir RDD yaratınız.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 7, 13, 15, 22, 36, 7, 11, 3, 25]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_s.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22, 36, 3, 7, 13, 15, 11, 25]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_s.distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. İkinci sorudaki rakamların liste içinde kaçar kez tekrarlandıklarını (frekanslarını) bulan bir Spark uygulaması yazınız.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 7, 13, 15, 22, 36, 7, 11, 3, 25]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_2=sc.parallelize([3,7,13,15,22,36,7,11,3,25])\n",
    "rdd_2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {3: 2, 7: 2, 13: 1, 15: 1, 22: 1, 36: 1, 11: 1, 25: 1})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_2.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 1),\n",
       " (7, 1),\n",
       " (13, 1),\n",
       " (15, 1),\n",
       " (22, 1),\n",
       " (36, 1),\n",
       " (7, 1),\n",
       " (11, 1),\n",
       " (3, 1),\n",
       " (25, 1)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_2.map(lambda x :(x,1)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 2), (7, 2), (11, 1), (13, 1), (15, 1), (22, 1), (25, 1), (36, 1)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_2.map(lambda x :(x,1)).reduceByKey(lambda x,y :x+y).sortByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(22, 1), (36, 1), (3, 2), (7, 2), (13, 1), (15, 1), (11, 1), (25, 1)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_2.map(lambda x :(x,1)).reduceByKey(lambda x,y :x+y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MAP vs FLATMAP Functions***<a class=\"anchor\" id=\"13\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Burada hata alırsanız komut satırından \"pip install findspark\" komutunu çalıştırarak findspark'ı yüklemeyi unutmayın.\n",
    "import findspark\n",
    "findspark.init(findspark.init(\"/Users/resitkadir/spark/spark-3.0.0/\"))\n",
    "import pyspark # only run after findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "#sc = SparkContext(\"local\",\"RDD-Olusturmak\")\n",
    "\n",
    "conf = SparkConf() \\\n",
    "        .setMaster(\"local[4]\") \\\n",
    "        .setAppName(\"RDD_Olusturmak\") \\\n",
    "        .setExecutorEnv(\"spark.executor.memory\", \"4g\") \\\n",
    "        .setExecutorEnv(\"spark.driver.memory\",\"2g\")\n",
    "\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter(lambda x: \"InvoiceNo\" not in x) ile başlık satırından kurtuluyoruz\n",
    "retailRDD = sc.textFile(\"datasets/OnlineRetail.csv\") \\\n",
    ".filter(lambda x: \"InvoiceNo\" not in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'536365;85123A;WHITE HANGING HEART T-LIGHT HOLDER;6;1.12.2010 08:26;2,55;17850;United Kingdom'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#İlk satırı görelim başlıktan kurtulmuş muyuz?\n",
    "retailRDD.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MAP Donusumu**\n",
    "\n",
    "     Quantity ile Unit price çarparak işlem tutarını bulmak ve InvoiceNo'dan C harflerini bularak yeni bir sütunda\n",
    "    işlemin iptal olup olmadığını boolean olarak yazmak\n",
    "    \n",
    "    map() içinde uygulayacağımız işlemleri bir fonksiyonda yazmak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(line):\n",
    "    isCancelled = True if(line.split(\";\")[0].startswith(\"C\")) else False\n",
    "    total = float(line.split(\";\")[3]) * float(line.split(\";\")[5].replace(\",\",\".\"))\n",
    "    return (isCancelled, total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "retailMapPriceRDD = retailRDD.map(my_func) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(False, 15.299999999999999), (False, 20.34), (False, 22.0)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retailMapPriceRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(True, -27.5),\n",
       " (True, -4.65),\n",
       " (True, -19.799999999999997),\n",
       " (True, -6.959999999999999),\n",
       " (True, -6.959999999999999),\n",
       " (True, -6.959999999999999),\n",
       " (True, -41.400000000000006),\n",
       " (True, -19.799999999999997),\n",
       " (True, -39.599999999999994),\n",
       " (True, -25.5)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#İptal oanları filtreleyelim\n",
    "retailMapPriceRDD.filter(lambda x: x[0] == True).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9288"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#iptal olanlari sayalim\n",
    "retailMapPriceRDD.filter(lambda x: x[0] == True).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4335272"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FLATMAP donusumu\n",
    "retailFlatMapSplittedRDD = retailRDD.flatMap(lambda x: x.split(\";\"))\n",
    "retailFlatMapSplittedRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retailRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['536365',\n",
       " '85123A',\n",
       " 'WHITE HANGING HEART T-LIGHT HOLDER',\n",
       " '6',\n",
       " '1.12.2010 08:26',\n",
       " '2,55',\n",
       " '17850',\n",
       " 'UNITED KINGDOM',\n",
       " '536365',\n",
       " '71053',\n",
       " 'WHITE METAL LANTERN',\n",
       " '6',\n",
       " '1.12.2010 08:26',\n",
       " '3,39',\n",
       " '17850']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#flatMap ile her kelimeyi büyük harf yapma\n",
    "retailFlatMapUpper = retailRDD.flatMap(lambda x: x.split(\";\")).map(lambda x: x.upper())\n",
    "retailFlatMapUpper.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-896812.4900000116]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## İptal edilen satışların toplam tutarı\n",
    "retailMapPriceRDD.reduceByKey(lambda x,y: x + y).take(2)\n",
    "\n",
    "retailMapPriceRDD.reduceByKey(lambda x,y: x + y) \\\n",
    "                .filter(lambda x: x[0] == True) \\\n",
    "                .map(lambda x: x[1]) \\\n",
    "                .take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(\"True\") if(\"ile\".startswith(\"e\")) else print(\"False\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ila'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ile\".replace(\"e\",\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(\"2,5\".replace(\",\",\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cancelled_price(line):\n",
    "    is_cancelled = True if(line.split(\";\")[0].startswith(\"C\")) else False\n",
    "    quantity = float(line.split(\";\")[3])\n",
    "    price = float(line.split(\";\")[5].replace(\",\",\".\"))\n",
    "    \n",
    "    total = quantity * price\n",
    "    return (is_cancelled, total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(False, 15.299999999999999),\n",
       " (False, 20.34),\n",
       " (False, 22.0),\n",
       " (False, 20.34),\n",
       " (False, 20.34)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retailTotal = retailRDD.map(cancelled_price)\n",
    "retailTotal.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-896812.4900000116]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reducedTotal = retailTotal.reduceByKey(lambda x,y:x+y)\n",
    "reducedTotal.filter(lambda x: x[0] == True).take(2)\n",
    "reducedTotal.filter(lambda x: x[0] == True).map(lambda x: x[1]).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD_Filter_Transformation <a class=\"anchor\" id=\"14\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(findspark.init(\"/Users/resitkadir/spark/spark-3.0.0/\"))\n",
    "# Create SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "# Aşağıdaki ayarları bilgisayarınızın belleğine göre değiştirebilirsiniz\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName(\"Dataset-Olusturmak\") \\\n",
    "        .config(\"spark.executor.memory\",\"4g\") \\\n",
    "        .config(\"spark.driver.memory\",\"2g\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# sparkContext'i kısaltmada tut\n",
    "sc = spark.sparkContext\n",
    "\n",
    "retailRDD = sc.textFile(\"datasets/OnlineRetail.csv\").filter(lambda x: 'InvoiceNo' not in x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['536365;85123A;WHITE HANGING HEART T-LIGHT HOLDER;6;1.12.2010 08:26;2,55;17850;United Kingdom',\n",
       " '536365;71053;WHITE METAL LANTERN;6;1.12.2010 08:26;3,39;17850;United Kingdom',\n",
       " '536365;84406B;CREAM CUPID HEARTS COAT HANGER;8;1.12.2010 08:26;2,75;17850;United Kingdom']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retailRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Header satırından kurtulma\n",
    "firstline = retailRDD.first()\n",
    "firstlinerdd = sc.parallelize([firstline])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "retailRDDWithoutHeader = retailRDD.subtract(firstlinerdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['536367;84969;BOX OF 6 ASSORTED COLOUR TEASPOONS;6;1.12.2010 08:34;4,25;13047;United Kingdom',\n",
       " '536369;21756;BATH BUILDING BLOCK WORD;3;1.12.2010 08:35;5,95;13047;United Kingdom',\n",
       " '536370;22326;ROUND SNACK BOXES SET OF4 WOODLAND;24;1.12.2010 08:45;2,95;12583;France',\n",
       " '536370;21731;RED TOADSTOOL LED NIGHT LIGHT;24;1.12.2010 08:45;1,65;12583;France',\n",
       " '536372;22632;HAND WARMER RED POLKA DOT;6;1.12.2010 09:01;1,85;17850;United Kingdom']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retailRDDWithoutHeader.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['536367;84969;BOX OF 6 ASSORTED COLOUR TEASPOONS;6;1.12.2010 08:34;4,25;13047;United Kingdom',\n",
       " '536367;84879;ASSORTED COLOUR BIRD ORNAMENT;32;1.12.2010 08:34;1,69;13047;United Kingdom',\n",
       " '536367;21755;LOVE BUILDING BLOCK WORD;3;1.12.2010 08:34;5,95;13047;United Kingdom',\n",
       " \"536367;22745;POPPY'S PLAYHOUSE BEDROOM;6;1.12.2010 08:34;2,1;13047;United Kingdom\",\n",
       " '536367;22310;IVORY KNITTED MUG COSY;6;1.12.2010 08:34;1,65;13047;United Kingdom',\n",
       " '536367;48187;DOORMAT NEW ENGLAND;4;1.12.2010 08:34;7,95;13047;United Kingdom',\n",
       " '536367;22623;BOX OF VINTAGE JIGSAW BLOCKS;3;1.12.2010 08:34;4,95;13047;United Kingdom',\n",
       " '536367;21754;HOME BUILDING BLOCK WORD;3;1.12.2010 08:34;5,95;13047;United Kingdom',\n",
       " \"536367;22748;POPPY'S PLAYHOUSE KITCHEN;6;1.12.2010 08:34;2,1;13047;United Kingdom\",\n",
       " '536367;22749;FELTCRAFT PRINCESS CHARLOTTE DOLL;8;1.12.2010 08:34;3,75;13047;United Kingdom']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### InvoiceNo 536367 olan siparişleri filtreleyelim\n",
    "# InvoiceNo değerini string yazarak \n",
    "retailRDDWithoutHeader.filter(lambda line: line.split(\";\")[0] == '536367').take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['536739;85159A;BLACK TEA,COFFEE,SUGAR JARS;2;2.12.2010 13:08;6,35;14180;United Kingdom',\n",
       " '536750;37370;RETRO COFFEE MUGS ASSORTED;6;2.12.2010 14:04;1,06;17850;United Kingdom',\n",
       " '536787;37370;RETRO COFFEE MUGS ASSORTED;6;2.12.2010 15:24;1,06;17850;United Kingdom',\n",
       " '536804;37370;RETRO COFFEE MUGS ASSORTED;72;2.12.2010 16:34;1,06;14031;United Kingdom',\n",
       " '536805;37370;RETRO COFFEE MUGS ASSORTED;12;2.12.2010 16:38;1,25;14775;United Kingdom',\n",
       " '536864;21216;SET 3 RETROSPOT TEA,COFFEE,SUGAR;1;3.12.2010 11:27;11,02;000000;United Kingdom',\n",
       " '536865;37370;RETRO COFFEE MUGS ASSORTED;1;3.12.2010 11:28;16,13;000000;United Kingdom',\n",
       " '537126;21216;SET 3 RETROSPOT TEA,COFFEE,SUGAR;1;5.12.2010 12:13;4,95;18118;United Kingdom',\n",
       " '537231;22304;COFFEE MUG BLUE PAISLEY DESIGN;6;6.12.2010 09:21;2,55;13652;United Kingdom',\n",
       " '537236;21216;SET 3 RETROSPOT TEA,COFFEE,SUGAR;8;6.12.2010 09:52;4,95;16858;United Kingdom',\n",
       " '537369;72122;COFFEE SCENT PILLAR CANDLE;1;6.12.2010 12:41;0,95;17860;United Kingdom',\n",
       " '537465;37370;RETRO COFFEE MUGS ASSORTED;12;7.12.2010 10:32;1,25;17735;United Kingdom',\n",
       " '537638;37370;RETRO COFFEE MUGS ASSORTED;1;7.12.2010 15:28;16,13;000000;United Kingdom',\n",
       " '537685;22301;COFFEE MUG CAT + BIRD DESIGN;6;8.12.2010 10:21;2,55;18077;United Kingdom',\n",
       " '537754;22304;COFFEE MUG BLUE PAISLEY DESIGN;6;8.12.2010 11:16;2,55;16081;United Kingdom',\n",
       " '537762;22301;COFFEE MUG CAT + BIRD DESIGN;6;8.12.2010 12:01;2,55;16558;United Kingdom',\n",
       " 'C537860;37370;RETRO COFFEE MUGS ASSORTED;-12;8.12.2010 16:15;1,25;16252;United Kingdom',\n",
       " '538035;37370;RETRO COFFEE MUGS ASSORTED;3;9.12.2010 13:03;1,25;16065;United Kingdom',\n",
       " '538071;37370;RETRO COFFEE MUGS ASSORTED;1;9.12.2010 14:09;16,13;000000;United Kingdom',\n",
       " '538880;21216;SET 3 RETROSPOT TEA,COFFEE,SUGAR;1;14.12.2010 15:52;11,02;000000;United Kingdom']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Ürün isimlerinden COFFEE içerenleri filtreleme\n",
    "# Ürün isimlerinden COFFEE içerenleri filtreleme\n",
    "retailRDDWithoutHeader.filter(lambda line: 'COFFEE' in line.split(\";\")[2]).take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['536367;84969;BOX OF 6 ASSORTED COLOUR TEASPOONS;6;1.12.2010 08:34;4,25;13047;United Kingdom',\n",
       " '536369;21756;BATH BUILDING BLOCK WORD;3;1.12.2010 08:35;5,95;13047;United Kingdom',\n",
       " '536370;22326;ROUND SNACK BOXES SET OF4 WOODLAND;24;1.12.2010 08:45;2,95;12583;France',\n",
       " '536370;21731;RED TOADSTOOL LED NIGHT LIGHT;24;1.12.2010 08:45;1,65;12583;France',\n",
       " '536372;22632;HAND WARMER RED POLKA DOT;6;1.12.2010 09:01;1,85;17850;United Kingdom']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Fiyatı 2000.0'den büyük alışverişler\n",
    "retailRDDWithoutHeader.filter(lambda line: float(line.split(\";\")[6]) > 2000.0).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bir fonksiyon ile filtreleme yapma \n",
    "# Quantity > 10\n",
    "def miktari_ondan_buyukler(x):\n",
    "    id = x.split(\";\")[3]\n",
    "    return int(id) > 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['536370;22326;ROUND SNACK BOXES SET OF4 WOODLAND;24;1.12.2010 08:45;2,95;12583;France',\n",
       " '536370;21731;RED TOADSTOOL LED NIGHT LIGHT;24;1.12.2010 08:45;1,65;12583;France',\n",
       " '536378;85183B;CHARLIE & LOLA WASTEPAPER BIN FLORA;48;1.12.2010 09:37;1,25;14688;United Kingdom',\n",
       " '536381;22719;GUMBALL MONOCHROME COAT RACK;36;1.12.2010 09:41;1,06;15311;United Kingdom',\n",
       " '536384;22470;HEART OF WICKER LARGE;40;1.12.2010 09:53;2,55;18074;United Kingdom']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retailRDDWithoutHeader.filter(lambda x: miktari_ondan_buyukler(x)).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Biraz daha karmaşık bir fonksiyon yazalım\n",
    "# Belirli bir tarihten sonra belli bir ülkede gerçekleşen işlemler\n",
    "import datetime\n",
    "# InvoiceNo;StockCode;Description;Quantity;InvoiceDate;UnitPrice;CustomerID;Country\n",
    "def daha_karmasik_filtre(x):\n",
    "    InvoiceNo = x.split(\";\")[0]\n",
    "    StockCode = x.split(\";\")[1]\n",
    "    Description = x.split(\";\")[2]\n",
    "    Quantity = x.split(\";\")[3]\n",
    "    InvoiceDate = x.split(\";\")[4]\n",
    "    UnitPrice = x.split(\";\")[5]\n",
    "    CustomerID = x.split(\";\")[6]\n",
    "    Country = x.split(\";\")[7]\n",
    "    \n",
    "    tarih = datetime.datetime.strptime(InvoiceDate, \"%d.%m.%Y %H:%M\")\n",
    "    \n",
    "    return tarih >= datetime.datetime(2010, 12, 1, 9, 58) and Country.startswith('United')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['536387;79321;CHILLI LIGHTS;192;1.12.2010 09:58;3,82;16029;United Kingdom',\n",
       " '536388;21411;GINGHAM HEART  DOORSTOP RED;3;1.12.2010 09:59;4,25;16250;United Kingdom',\n",
       " '536388;22922;FRIDGE MAGNETS US DINER ASSORTED;12;1.12.2010 09:59;0,85;16250;United Kingdom',\n",
       " '536388;22469;HEART OF WICKER SMALL;12;1.12.2010 09:59;1,65;16250;United Kingdom',\n",
       " '536388;22242;5 HOOK HANGER MAGIC TOADSTOOL;12;1.12.2010 09:59;1,65;16250;United Kingdom',\n",
       " '536390;22960;JAM MAKING SET WITH JARS;12;1.12.2010 10:19;3,75;17511;United Kingdom',\n",
       " '536390;20668;DISCO BALL CHRISTMAS DECORATION;288;1.12.2010 10:19;0,1;17511;United Kingdom',\n",
       " '536390;22197;SMALL POPCORN HOLDER;100;1.12.2010 10:19;0,72;17511;United Kingdom',\n",
       " '536390;21786;POLKADOT RAIN HAT;144;1.12.2010 10:19;0,32;17511;United Kingdom',\n",
       " '536390;22174;PHOTO CUBE;48;1.12.2010 10:19;1,48;17511;United Kingdom']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retailRDDWithoutHeader.filter(lambda x: daha_karmasik_filtre(x)).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD_Join-01 <a class=\"anchor\" id=\"15\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['11,5,1014,2,99.96,49.98',\n",
       " '12,5,957,1,299.98,299.98',\n",
       " '13,5,403,1,129.99,129.99',\n",
       " '14,7,1073,1,199.99,199.99',\n",
       " '15,7,957,1,299.98,299.98']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init(findspark.init(\"/Users/resitkadir/spark/spark-3.0.0/\"))\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"RDDJoin\").setMaster(\"local[4]\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "#Read data\n",
    "# order_items okuma ve başlıktan kurtulma\n",
    "order_items_rdd = sc.textFile(\"datasets/retail_db/order_items.csv\") \\\n",
    "                    .filter(lambda x: \"orderItemName\" not in x) \\\n",
    "                    .repartition(4)\n",
    "\n",
    "order_items_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['11,2,Fitness Gear 300 lb Olympic Weight Set,,209.99,http://images.acmesports.sports/Fitness+Gear+300+lb+Olympic+Weight+Set',\n",
       " \"12,2,Under Armour Men's Highlight MC Alter Ego Fla,,139.99,http://images.acmesports.sports/Under+Armour+Men%27s+Highlight+MC+Alter+Ego+Flash+Football...\",\n",
       " \"13,2,Under Armour Men's Renegade D Mid Football Cl,,89.99,http://images.acmesports.sports/Under+Armour+Men%27s+Renegade+D+Mid+Football+Cleat\",\n",
       " '14,2,Quik Shade Summit SX170 10 FT. x 10 FT. Canop,,199.99,http://images.acmesports.sports/Quik+Shade+Summit+SX170+10+FT.+x+10+FT.+Canopy',\n",
       " \"15,2,Under Armour Kids' Highlight RM Alter Ego Sup,,59.99,http://images.acmesports.sports/Under+Armour+Kids%27+Highlight+RM+Alter+Ego+Superman+Football...\"]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# products okuma ve başlıktan kurtulma\n",
    "products_rdd = sc.textFile(\"datasets/retail_db/products.csv\") \\\n",
    "        .filter(lambda x: \"productDescription\" not in x) \\\n",
    "            .repartition(4)\n",
    "\n",
    "products_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1014', ('11', '5', '2', '99.96', '49.98')),\n",
       " ('957', ('12', '5', '1', '299.98', '299.98')),\n",
       " ('403', ('13', '5', '1', '129.99', '129.99')),\n",
       " ('1073', ('14', '7', '1', '199.99', '199.99')),\n",
       " ('957', ('15', '7', '1', '299.98', '299.98'))]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OKUNAN VERİLERİ PAIR RDD'ye ÇEVİRME SAFHASI\n",
    "\n",
    "# order_items pair_rdd yapma\n",
    "def make_order_items_pair_rdd(line):\n",
    "    orderItemName = line.split(\",\")[0]\n",
    "    orderItemOrderId = line.split(\",\")[1]\n",
    "    orderItemProductId = line.split(\",\")[2]\n",
    "    orderItemQuantity = line.split(\",\")[3]\n",
    "    orderItemSubTotal = line.split(\",\")[4]\n",
    "    orderItemProductPrice = line.split(\",\")[5]\n",
    "    \n",
    "    return (orderItemProductId, (orderItemName, orderItemOrderId, orderItemQuantity, \n",
    "                                 orderItemSubTotal,orderItemProductPrice))\n",
    "order_item_pair_rdd = order_items_rdd.map(make_order_items_pair_rdd)\n",
    "order_item_pair_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('11',\n",
       "  ('2',\n",
       "   'Fitness Gear 300 lb Olympic Weight Set',\n",
       "   '',\n",
       "   '209.99',\n",
       "   'http://images.acmesports.sports/Fitness+Gear+300+lb+Olympic+Weight+Set')),\n",
       " ('12',\n",
       "  ('2',\n",
       "   \"Under Armour Men's Highlight MC Alter Ego Fla\",\n",
       "   '',\n",
       "   '139.99',\n",
       "   'http://images.acmesports.sports/Under+Armour+Men%27s+Highlight+MC+Alter+Ego+Flash+Football...'))]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# products için pair rdd yapma\n",
    "def make_products_pair_rdd(line):\n",
    "    productId = line.split(\",\")[0]\n",
    "    productCategoryId = line.split(\",\")[1]\n",
    "    productName = line.split(\",\")[2]\n",
    "    productDescription = line.split(\",\")[3]\n",
    "    productPrice = line.split(\",\")[4]\n",
    "    productImage = line.split(\",\")[5]\n",
    "    \n",
    "    return (productId,(productCategoryId, productName, productDescription, productPrice, productImage))\n",
    "\n",
    "products_pair_rdd = products_rdd.map(make_products_pair_rdd)\n",
    "products_pair_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('957',\n",
       "  (('12', '5', '1', '299.98', '299.98'),\n",
       "   ('43',\n",
       "    \"Diamondback Women's Serene Classic Comfort Bi\",\n",
       "    '',\n",
       "    '299.98',\n",
       "    'http://images.acmesports.sports/Diamondback+Women%27s+Serene+Classic+Comfort+Bike+2014'))),\n",
       " ('957',\n",
       "  (('15', '7', '1', '299.98', '299.98'),\n",
       "   ('43',\n",
       "    \"Diamondback Women's Serene Classic Comfort Bi\",\n",
       "    '',\n",
       "    '299.98',\n",
       "    'http://images.acmesports.sports/Diamondback+Women%27s+Serene+Classic+Comfort+Bike+2014')))]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# JOIN AŞAMASI\n",
    "order_items_product_pair_rdd = order_item_pair_rdd.join(products_pair_rdd)\n",
    "order_items_product_pair_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAIR RDD Operations <a class=\"anchor\" id=\"16\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1,Cemal,35,Isci,Ankara,3500'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Burada hata alırsanız komut satırından \"pip install findspark\" komutunu çalıştırarak findspark'ı yüklemeyi unutmayın.\n",
    "import findspark\n",
    "findspark.init(findspark.init(\"/Users/resitkadir/spark/spark-3.0.0/\"))\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "#########################\n",
    "sc = SparkContext(\"local[4]\",\"PairRDDD-Ops\")\n",
    "##################################\n",
    "insanlarRDD2 = sc.textFile(\"datasets/simple_data.csv\")\n",
    "insanlarRDD = insanlarRDD2.filter(lambda x: \"sirano\" not in x)\n",
    "insanlarRDD.first()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Isci', 3500.0), ('Memur', 4200.0), ('Müzisyen', 9000.0)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def meslek_maas(line):\n",
    "    meslek = line.split(\",\")[3]\n",
    "    maas = float(line.split(\",\")[5])\n",
    "    \n",
    "    return (meslek,maas)\n",
    "\n",
    "meslek_maas_pairRDD = insanlarRDD.map(meslek_maas)\n",
    "meslek_maas_pairRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Isci', (3500.0, 1)),\n",
       " ('Memur', (4200.0, 1)),\n",
       " ('Müzisyen', (9000.0, 1)),\n",
       " ('Pazarlamaci', (4200.0, 1)),\n",
       " ('Pazarlamaci', (4800.0, 1))]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meslek_maas = meslek_maas_pairRDD.mapValues(lambda x: (x,1))\n",
    "meslek_maas.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Memur', (12200.0, 3)),\n",
       " ('Pazarlamaci', (16300.0, 3)),\n",
       " ('Tuhafiyeci', (4800.0, 1)),\n",
       " ('Tornacı', (4200.0, 1)),\n",
       " ('Isci', (3500.0, 1))]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meslek_maas_RBK = meslek_maas.reduceByKey(lambda x,y: (x[0] + y[0], x[1] + y[1]))\n",
    "meslek_maas_RBK.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Memur', 4066.6666666666665),\n",
       " ('Pazarlamaci', 5433.333333333333),\n",
       " ('Tuhafiyeci', 4800.0),\n",
       " ('Tornacı', 4200.0),\n",
       " ('Isci', 3500.0),\n",
       " ('Müzisyen', 9900.0),\n",
       " ('Doktor', 16125.0),\n",
       " ('Berber', 12000.0)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meslek_ort_maas = meslek_maas_RBK.mapValues(lambda x: x[0] / x[1]) \n",
    "meslek_ort_maas.take(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excel-Dataframe-RDD <a class=\"anchor\" id=\"17\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sirano=1, isim='Cemal', yas=35, meslek='Isci', sehir='Ankara', aylik_gelir=3500),\n",
       " Row(sirano=2, isim='Ceyda', yas=42, meslek='Memur', sehir='Kayseri', aylik_gelir=4200),\n",
       " Row(sirano=3, isim='Timur', yas=30, meslek='MÃ¼zisyen', sehir='Istanbul', aylik_gelir=9000),\n",
       " Row(sirano=4, isim='Burcu', yas=29, meslek='Pazarlamaci', sehir='Ankara', aylik_gelir=4200),\n",
       " Row(sirano=5, isim='Yasemin', yas=23, meslek='Pazarlamaci', sehir='Bursa', aylik_gelir=4800)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init(findspark.init(\"/Users/resitkadir/spark/spark-3.0.0/\"))\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "spark = SparkSession.builder.master(\"local[4]\").appName(\"Lung_Cancer\").getOrCreate()\n",
    "\n",
    "pdf = pd.read_excel(\"datasets/simple_data.xlsx\")\n",
    "df_pd = spark.createDataFrame(pdf)\n",
    "rdd = df_pd.rdd\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BroadcastVariablesOps<a class=\"anchor\" id=\"18\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Nike Men's Fly Shorts 2.0\""
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init(findspark.init(\"/Users/resitkadir/spark/spark-3.0.0/\"))\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setMaster(\"local[4]\").setAppName(\"BroadcastVariablesOps\")\n",
    "sc = SparkContext(conf=conf).getOrCreate()\n",
    "\n",
    "# products.csv dosyasını okuyup (urün_id, ürün_adı) döndüren fonksiyon\n",
    "\n",
    "def read_products():\n",
    "    products_text_wrapper = open(\"datasets/retail_db/products.csv\", \"r\",encoding=\"utf-8\")\n",
    "    # satır satır okuma\n",
    "    products = products_text_wrapper.readlines()\n",
    "    \n",
    "    product_id_name = {}\n",
    "    \n",
    "    for line in products:\n",
    "        # başlık satırını atlamak için if kontrolü\n",
    "        if \"productName\" not in line:\n",
    "            product_id = int(line.split(\",\")[0])\n",
    "            product_name = line.split(\",\")[2]\n",
    "            # product_id_name.append((product_id,product_name))\n",
    "            product_id_name.update({product_id: product_name})\n",
    "    return product_id_name\n",
    "\n",
    "products = read_products()\n",
    "broadcast_products = sc.broadcast(products)\n",
    "broadcast_products.value.get(114)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,1,957,1,299.98,299.98',\n",
       " '2,2,1073,1,199.99,199.99',\n",
       " '3,2,502,5,250.0,50.0',\n",
       " '4,2,403,1,129.99,129.99',\n",
       " '5,4,897,2,49.98,24.99']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# order_item okuma ve rdd oluşturma\n",
    "order_items_rdd = sc.textFile(\"datasets/retail_db/order_items.csv\") \\\n",
    "                    .filter(lambda x: \"orderItemOrderId\" not in x)\n",
    "\n",
    "order_items_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(957, 299.98), (1073, 199.99), (502, 250.0), (403, 129.99), (897, 49.98)]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# order_items pair_rdd yapma\n",
    "def make_order_items_pair_rdd(line):\n",
    "    order_item_product_id = int(line.split(\",\")[2])\n",
    "    order_item_sub_total = float(line.split(\",\")[4])\n",
    "    \n",
    "    return (order_item_product_id, order_item_sub_total)\n",
    "\n",
    "order_items_pair_rdd = order_items_rdd.map(make_order_items_pair_rdd)\n",
    "order_items_pair_rdd.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_orders = order_items_pair_rdd.reduceByKey(lambda x,y: x+y) \\\n",
    "            .map(lambda x: (x[1], x[0])) \\\n",
    "            .sortByKey(False) \\\n",
    "            .map(lambda x: (x[1], x[0])) \\\n",
    "            #.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1004, 6929653.499999708),\n",
       " (365, 4421143.019999639),\n",
       " (957, 4118425.419999785),\n",
       " (191, 3667633.1999997487),\n",
       " (502, 3147800.0)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_orders.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Field & Stream Sportsman 16 Gun Fire Safe', 6929653.499999708),\n",
       " ('Perfect Fitness Perfect Rip Deck', 4421143.019999639),\n",
       " (\"Diamondback Women's Serene Classic Comfort Bi\", 4118425.419999785),\n",
       " (\"Nike Men's Free 5.0+ Running Shoe\", 3667633.1999997487),\n",
       " (\"Nike Men's Dri-FIT Victory Golf Polo\", 3147800.0)]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# order_items ile broadcast variable olan products birleştirme\n",
    "sorted_orders_with_product_name = sorted_orders \\\n",
    "                                .map(lambda x: (broadcast_products.value.get(x[0]), x[1]))\n",
    "\n",
    "sorted_orders_with_product_name.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD_Wordcount<a class=\"anchor\" id=\"19\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ömer Seyfettin -         Forsa',\n",
       " '',\n",
       " 'Akdeniz’in, kahramanlık yuvası sonsuz ufuklarına bakan küçük tepe, minimini bir çiçek ',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init(findspark.init(\"/Users/resitkadir/spark/spark-3.0.0/\"))\n",
    "\n",
    "# Create SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "# Aşağıdaki ayarları bilgisayarınızın belleğine göre değiştirebilirsiniz\n",
    "spark = SparkSession.builder \\\n",
    ".master(\"local[4]\") \\\n",
    ".appName(\"RDD-Olusturmak\") \\\n",
    ".config(\"spark.executor.memory\",\"4g\") \\\n",
    ".config(\"spark.driver.memory\",\"2g\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "# sparkContext'i kısaltmada tut\n",
    "sc = spark.sparkContext\n",
    "#readfile\n",
    "veri_dosyasi = \"datasets/omer_seyfettin_forsa_hikaye.txt\"\n",
    "\n",
    "hikaye_rdd = sc.textFile(veri_dosyasi)\n",
    "hikaye_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "840"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Her bir kelimeyi boşluklarla ayıralım ve başka bir rdd'de tutalım\n",
    "kelimeler = hikaye_rdd.flatMap(lambda satir: satir.split(\" \"))\n",
    "# Kelimeleri sayalım\n",
    "kelime_sayilari = kelimeler.map(lambda kelime: (kelime,1)).reduceByKey(lambda x,y: x+y)\n",
    "# kaç farklı kelime var\n",
    "kelime_sayilari.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ömer', 1),\n",
       " ('Seyfettin', 1),\n",
       " ('', 85),\n",
       " ('Forsa', 1),\n",
       " ('Akdeniz’in,', 1),\n",
       " ('kahramanlık', 2),\n",
       " ('sonsuz', 1),\n",
       " ('ufuklarına', 1),\n",
       " ('bakan', 1),\n",
       " ('uzun', 1),\n",
       " ('badem', 1),\n",
       " ('alaca', 1),\n",
       " ('inen', 1),\n",
       " ('keçiyoluna', 1),\n",
       " ('rüzgârıyla', 1)]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kelimeler ve tekrarlanma sayılarından rastgel 15 tanesini görelim\n",
    "kelime_sayilari.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rakamları anahtar olarak kullanmak için 0 indisine kelimeleri 1 indisine atalım\n",
    "kelime_sayilari2 = kelime_sayilari.map(lambda x: (x[1], x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(85, ''),\n",
       " (32, 'bir'),\n",
       " (31, '–'),\n",
       " (8, 'yıl'),\n",
       " (6, 'diye'),\n",
       " (5, 'Türk'),\n",
       " (5, 'dedi.'),\n",
       " (5, 'onun'),\n",
       " (5, 'doğru'),\n",
       " (5, 'Kırk'),\n",
       " (4, 'Yirmi'),\n",
       " (4, 'tutsak'),\n",
       " (4, 'Ben'),\n",
       " (4, 'gibi'),\n",
       " (4, 'Ama'),\n",
       " (4, 'büyük'),\n",
       " (3, 'yanı'),\n",
       " (3, 'şey'),\n",
       " (3, 'onu'),\n",
       " (3, 'geminin')]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rakamlar artık key olunca saydoralım bakalım en çok tekrarlanan 15 kelime ne imiş\n",
    "kelime_sayilari2.sortByKey(False).take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AND OTHERS <a class=\"anchor\" id=\"20\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![key_value](IMG/key_value.png)\n",
    "**yasi 30 dan kucuk olanlari almak icin**\n",
    "![exa](IMG/exa_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ahmet', 35), ('oscar', 22), ('jason', 98)]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#**KEY-VALUE**\n",
    "\n",
    "import findspark\n",
    "findspark.init(findspark.init(\"/Users/resitkadir/spark/spark-2.4.6/\"))\n",
    "\n",
    "# Create SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "# Aşağıdaki ayarları bilgisayarınızın belleğine göre değiştirebilirsiniz\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName(\"Dataset-Olusturmak\") \\\n",
    "        .config(\"spark.executor.memory\",\"4g\") \\\n",
    "        .config(\"spark.driver.memory\",\"2g\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# sparkContext'i kısaltmada tut\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "ages = [(\"ahmet\",35),(\"oscar\",22),(\"jason\",98)]\n",
    "ages_rdd = sc.parallelize(ages)\n",
    "ages_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('oscar', 22)]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ages_rdd.filter(lambda key_value : key_value[1] < 30).take(3)\n",
    "#key_value nn birinci elemani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('oscar', 22)]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ages_rdd.filter(lambda key_value : key_value[0] == \"oscar\").take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
